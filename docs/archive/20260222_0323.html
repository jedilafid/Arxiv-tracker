<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-22 03:23</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260222_0323</div>
    <div class="row"><div class="card">
<div class="title">Reverso: Efficient Time Series Foundation Models for Zero-shot Forecasting</div>
<div class="meta-line">Authors: Xinghong Fu, Yanhong Li, Georgios Papaioannou, Yoon Kim</div>
<div class="meta-line">First: 2026-02-19T18:48:08+00:00 · Latest: 2026-02-19T18:48:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17634v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17634v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning time series foundation models has been shown to be a promising approach for zero-shot time series forecasting across diverse time series domains. Insofar as scaling has been a critical driver of performance of foundation models in other modalities such as language and vision, much recent work on time series foundation modeling has focused on scaling. This has resulted in time series foundation models with hundreds of millions of parameters that are, while performant, inefficient and expensive to use in practice. This paper describes a simple recipe for learning efficient foundation models for zero-shot time series forecasting that are orders of magnitude smaller. We show that large-scale transformers are not necessary: small hybrid models that interleave long convolution and linear RNN layers (in particular DeltaNet layers) can match the performance of larger transformer-based models while being more than a hundred times smaller. We also describe several data augmentation and inference strategies that further improve performance. This recipe results in Reverso, a family of efficient time series foundation models for zero-shot forecasting that significantly push the performance-efficiency Pareto frontier.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Learning time series foundation models has been shown to be a promising approach for zero-shot time series forecasting across diverse time series domains.</div>
</details>
</div>
<div class="card">
<div class="title">Be Wary of Your Time Series Preprocessing</div>
<div class="meta-line">Authors: Sofiane Ennadir, Tianze Wang, Oleg Smirnov, Sahar Asadi, Lele Cao</div>
<div class="meta-line">Venue: AAAI</div>
<div class="meta-line">First: 2026-02-19T17:23:56+00:00 · Latest: 2026-02-19T17:23:56+00:00</div>
<div class="meta-line">Comments: Accepted at the AI4TS workshop at AAAI-26</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17568v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17568v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Normalization and scaling are fundamental preprocessing steps in time series modeling, yet their role in Transformer-based models remains underexplored from a theoretical perspective. In this work, we present the first formal analysis of how different normalization strategies, specifically instance-based and global scaling, impact the expressivity of Transformer-based architectures for time series representation learning. We propose a novel expressivity framework tailored to time series, which quantifies a model&#x27;s ability to distinguish between similar and dissimilar inputs in the representation space. Using this framework, we derive theoretical bounds for two widely used normalization methods: Standard and Min-Max scaling. Our analysis reveals that the choice of normalization strategy can significantly influence the model&#x27;s representational capacity, depending on the task and data characteristics. We complement our theory with empirical validation on classification and forecasting benchmarks using multiple Transformer-based models. Our results show that no single normalization method consistently outperforms others, and in some cases, omitting normalization entirely leads to superior performance. These findings highlight the critical role of preprocessing in time series learning and motivate the need for more principled normalization strategies tailored to specific tasks and datasets.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Normalization and scaling are fundamental preprocessing steps in time series modeling, yet their role in Transformer-based models remains underexplored from a theoretical perspective.</div>
</details>
</div>
<div class="card">
<div class="title">Time-Varyingness in Auction Breaks Revenue Equivalence</div>
<div class="meta-line">Authors: Yuma Fujimoto, Kaito Ariu, Kenshi Abe</div>
<div class="meta-line">First: 2024-10-16T07:18:25+00:00 · Latest: 2026-02-19T14:39:36+00:00</div>
<div class="meta-line">Comments: 8 pages, 4 figures (main); 6 pages, 1 figure (appendix)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.12306v2">Abs</a> · <a href="https://arxiv.org/pdf/2410.12306v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Auction is applied for trade with various mechanisms. A simple but practical question is which mechanism, typically first-price or second-price auctions, is preferred from the perspective of bidders or sellers. A celebrated answer is revenue equivalence, where each bidder&#x27;s equilibrium payoff is proven to be independent of auction mechanisms (and a seller&#x27;s revenue, too). In reality, however, auction environments like the value distribution of items would vary over time, and such equilibrium bidding cannot always be achieved. Indeed, bidders must continue to track their equilibrium bidding by learning in first-price auctions, but they can keep their equilibrium bidding in second-price auctions. This study discusses whether and how revenue equivalence is violated in the long run by comparing the time series of non-equilibrium bidding in first-price auctions with those of equilibrium bidding in second-price auctions. We characterize the value distribution by two parameters: its basis value, which means the lowest price to bid, and its value interval, which means the width of possible values. Surprisingly, our theorems and experiments find that revenue equivalence is broken by the correlation between the basis value and the value interval, uncovering a novel phenomenon that could occur in the real world.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Auction is applied for trade with various mechanisms.</div>
</details>
</div>
<div class="card">
<div class="title">HPMixer: Hierarchical Patching for Multivariate Time Series Forecasting</div>
<div class="meta-line">Authors: Jung Min Choi, Vijaya Krishna Yalavarthi, Lars Schmidt-Thieme</div>
<div class="meta-line">First: 2026-02-18T13:59:04+00:00 · Latest: 2026-02-19T12:57:13+00:00</div>
<div class="meta-line">Comments: 18 pages, 5 figures, 5 tables, PAKDD 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16468v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.16468v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In long-term multivariate time series forecasting, effectively capturing both periodic patterns and residual dynamics is essential. To address this within standard deep learning benchmark settings, we propose the Hierarchical Patching Mixer (HPMixer), which models periodicity and residuals in a decoupled yet complementary manner. The periodic component utilizes a learnable cycle module [7] enhanced with a nonlinear channel-wise MLP for greater expressiveness. The residual component is processed through a Learnable Stationary Wavelet Transform (LSWT) to extract stable, shift-invariant frequency-domain representations. Subsequently, a channel-mixing encoder models explicit inter-channel dependencies, while a two-level non-overlapping hierarchical patching mechanism captures coarse- and fine-scale residual variations. By integrating decoupled periodicity modeling with structured, multi-scale residual learning, HPMixer provides an effective framework. Extensive experiments on standard multivariate benchmarks demonstrate that HPMixer achieves competitive or state-of-the-art performance compared to recent baselines.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In long-term multivariate time series forecasting, effectively capturing both periodic patterns and residual dynamics is essential.</div>
</details>
</div>
<div class="card">
<div class="title">MGD: Moment Guided Diffusion for Maximum Entropy Generation</div>
<div class="meta-line">Authors: Etienne Lempereur, Nathanaël Cuvelle--Magar, Florentin Coeurdoux, Stéphane Mallat, Eric Vanden-Eijnden</div>
<div class="meta-line">First: 2026-02-19T10:03:03+00:00 · Latest: 2026-02-19T10:03:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17211v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17211v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generating samples from limited information is a fundamental problem across scientific domains. Classical maximum entropy methods provide principled uncertainty quantification from moment constraints but require sampling via MCMC or Langevin dynamics, which typically exhibit exponential slowdown in high dimensions. In contrast, generative models based on diffusion and flow matching efficiently transport noise to data but offer limited theoretical guarantees and can overfit when data is scarce. We introduce Moment Guided Diffusion (MGD), which combines elements of both approaches. Building on the stochastic interpolant framework, MGD samples maximum entropy distributions by solving a stochastic differential equation that guides moments toward prescribed values in finite time, thereby avoiding slow mixing in equilibrium-based methods. We formally obtain, in the large-volatility limit, convergence of MGD to the maximum entropy distribution and derive a tractable estimator of the resulting entropy computed directly from the dynamics. Applications to financial time series, turbulent flows, and cosmological fields using wavelet scattering moments yield estimates of negentropy for high-dimensional multiscale processes.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Generating samples from limited information is a fundamental problem across scientific domains.</div>
</details>
</div>
<div class="card">
<div class="title">Exploring Memory Effects: Sparse Identification in Vector-Borne Diseases</div>
<div class="meta-line">Authors: Dimitri Breda, Muhammad Tanveer, Jianhong Wu, Xue Zhang</div>
<div class="meta-line">First: 2026-01-28T13:26:43+00:00 · Latest: 2026-02-19T10:02:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20591v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.20591v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Predicting the human burden of vector-borne diseases from limited surveillance data remains a major challenge, particularly in the presence of nonlinear transmission dynamics and delayed effects arising from vector ecology and human behavior. We develop a data-driven framework based on an extension of Sparse Identification of Nonlinear Dynamics (SINDy) to systems with distributed memory, enabling discovery of transmission mechanisms directly from time series data. Using severe fever with thrombocytopenia syndrome (SFTS) as a case study, we show that this approach can uncover key features of tick-borne disease dynamics using only human incidence and local temperature data, without imposing predefined assumptions on human case reporting. We further demonstrate that predictive performance is substantially enhanced when the data-driven model is coupled with mechanistic representations of tick-host transmission pathways informed by empirical studies. The framework supports systematic sensitivity analysis of memory kernels and behavioral parameters, identifying those most influential for prediction accuracy. Although the approach prioritizes predictive accuracy over mechanistic transparency, it yields sparse, interpretable integral representations suitable for epidemiological forecasting. This hybrid methodology provides a scalable strategy for forecasting vector-borne disease risk and informing public health decision-making under data limitations.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Predicting the human burden of vector-borne diseases from limited surveillance data remains a major challenge, particularly in the presence of nonlinear transmission dynamics and delayed effects arising from vector ecology and human behavior.</div>
</details>
</div>
<div class="card">
<div class="title">TimeOmni-VL: Unified Models for Time Series Understanding and Generation</div>
<div class="meta-line">Authors: Tong Guan, Sheng Pan, Johan Barthelemy, Zhao Li, Yujun Cai, Cesare Alippi, Ming Jin, Shirui Pan</div>
<div class="meta-line">First: 2026-02-19T07:50:11+00:00 · Latest: 2026-02-19T07:50:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17149v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17149v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent time series modeling faces a sharp divide between numerical generation and semantic understanding, with research showing that generation models often rely on superficial pattern matching, while understanding-oriented models struggle with high-fidelity numerical output. Although unified multimodal models (UMMs) have bridged this gap in vision, their potential for time series remains untapped. We propose TimeOmni-VL, the first vision-centric framework that unifies time series understanding and generation through two key innovations: (1) Fidelity-preserving bidirectional mapping between time series and images (Bi-TSI), which advances Time Series-to-Image (TS2I) and Image-to-Time Series (I2TS) conversions to ensure near-lossless transformations. (2) Understanding-guided generation. We introduce TSUMM-Suite, a novel dataset consists of six understanding tasks rooted in time series analytics that are coupled with two generation tasks. With a calibrated Chain-of-Thought, TimeOmni-VL is the first to leverage time series understanding as an explicit control signal for high-fidelity generation. Experiments confirm that this unified approach significantly improves both semantic understanding and numerical precision, establishing a new frontier for multimodal time series modeling.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent time series modeling faces a sharp divide between numerical generation and semantic understanding, with research showing that generation models often rely on superficial pattern matching, while understanding-oriented models struggle with high-fidelity numerical output.</div>
</details>
</div>
<div class="card">
<div class="title">Assessing Ionospheric Scintillation Risk for Direct-to-Cellular Communications using Frequency-Scaled GNSS Observations</div>
<div class="meta-line">Authors: Abdollah Masoud Darya, Muhammad Mubasshir Shaikh</div>
<div class="meta-line">First: 2026-02-19T07:41:27+00:00 · Latest: 2026-02-19T07:41:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17143v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17143v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">One of the key issues facing Direct-to-Cellular (D2C) satellite communication systems is ionospheric scintillation on the uplink and downlink, which can significantly degrade link quality. This work investigates the spatial and temporal characteristics of amplitude scintillation at D2C frequencies by scaling L-band scintillation observations from Global Navigation Satellite Systems (GNSS) receivers to bands relevant to D2C operation, including the low-band, and 3GPP&#x27;s N255 and N256. These observations are then compared to scaled radio-occultation scintillation observations from the FORMOSAT-7/COSMIC-2 (F7/C2) mission, which can be used in regions that do not possess ground-based scintillation monitoring stations. As a proof of concept, five years of ground-based GNSS scintillation data from Sharjah, United Arab Emirates, together with two years of F7/C2 observations over the same region, corresponding to the ascending phase of Solar Cycle 25, are analyzed. Both space-based and ground-based observations indicate a pronounced diurnal scintillation peak between 20--22 local time, particularly during the equinoxes, with occurrence rates increasing with solar activity. Ground-based observations also reveal a strong azimuth dependence, with most scintillation events occurring on southward satellite links. The scintillation occurrence rate at the low-band is more than twice that observed at N255 and N256, highlighting the increased robustness of higher D2C bands to ionospheric scintillation. These results demonstrate how GNSS scintillation observations can be leveraged to characterize and anticipate scintillation-induced D2C link impairments, which help in D2C system design and the implementation of scintillation mitigation strategies.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">One of the key issues facing Direct-to-Cellular (D2C) satellite communication systems is ionospheric scintillation on the uplink and downlink, which can significantly degrade link quality.</div>
</details>
</div>
<div class="card">
<div class="title">TIFO: Time-Invariant Frequency Operator for Stationarity-Aware Representation Learning in Time Series</div>
<div class="meta-line">Authors: Xihao Piao, Zheng Chen, Lingwei Zhu, Yushun Dong, Yasuko Matsubara, Yasushi Sakurai</div>
<div class="meta-line">First: 2026-02-19T06:46:54+00:00 · Latest: 2026-02-19T06:46:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17122v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17122v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Nonstationary time series forecasting suffers from the distribution shift issue due to the different distributions that produce the training and test data. Existing methods attempt to alleviate the dependence by, e.g., removing low-order moments from each individual sample. These solutions fail to capture the underlying time-evolving structure across samples and do not model the complex time structure. In this paper, we aim to address the distribution shift in the frequency space by considering all possible time structures. To this end, we propose a Time-Invariant Frequency Operator (TIFO), which learns stationarity-aware weights over the frequency spectrum across the entire dataset. The weight representation highlights stationary frequency components while suppressing non-stationary ones, thereby mitigating the distribution shift issue in time series. To justify our method, we show that the Fourier transform of time series data implicitly induces eigen-decomposition in the frequency space. TIFO is a plug-and-play approach that can be seamlessly integrated into various forecasting models. Experiments demonstrate our method achieves 18 top-1 and 6 top-2 results out of 28 forecasting settings. Notably, it yields 33.3% and 55.3% improvements in average MSE on the ETTm2 dataset. In addition, TIFO reduces computational costs by 60% -70% compared to baseline methods, demonstrating strong scalability across diverse forecasting models.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Nonstationary time series forecasting suffers from the distribution shift issue due to the different distributions that produce the training and test data.</div>
</details>
</div>
<div class="card">
<div class="title">AI/ML based Joint Source and Channel Coding for HARQ-ACK Payload</div>
<div class="meta-line">Authors: Akash Doshi, Pinar Sen, Kirill Ivanov, Wei Yang, June Namgoong, Runxin Wang, Rachel Wang, Taesang Yoo, Jing Jiang, Tingfang Ji</div>
<div class="meta-line">First: 2025-11-25T05:31:26+00:00 · Latest: 2026-02-19T06:11:06+00:00</div>
<div class="meta-line">Comments: 39 pages, 15 figures. Under consideration for publication in Journal of Sel. Areas in Information Theory (received Major Revision). This paper was presented in part at the International Symposium on Topics in Coding, August 2025 in the Session for Coding and AI</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.19943v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.19943v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Channel coding from 2G to 5G has assumed the inputs bits at the physical layer to be uniformly distributed. However, hybrid automatic repeat request acknowledgement (HARQ-ACK) bits transmitted in the uplink are inherently non-uniformly distributed. For such sources, significant performance gains could be obtained by employing joint source channel coding, aided by deep learning-based techniques. In this paper, we learn a transformer-based encoder using a novel &quot;free-lunch&quot; training algorithm and propose per-codeword power shaping to exploit the source prior at the encoder whilst being robust to small changes in the HARQ-ACK distribution. Furthermore, any HARQ-ACK decoder has to achieve a low negative acknowledgement (NACK) error rate to avoid radio link failures resulting from multiple NACK errors. We develop an extension of the Neyman-Pearson test to a coded bit system with multiple information bits to achieve Unequal Error Protection of NACK over ACK bits at the decoder. Finally, we apply the proposed encoder and decoder designs to a 5G New Radio (NR) compliant uplink setup under a fading channel, describing the optimal receiver design and a low complexity coherent approximation to it. Our results demonstrate 3-6 dB reduction in the average transmit power required to achieve the target error rates compared to the NR baseline, while also achieving a 2-3 dB reduction in the maximum transmit power, thus providing for significant coverage gains and power savings.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Channel coding from 2G to 5G has assumed the inputs bits at the physical layer to be uniformly distributed.</div>
</details>
</div>
<div class="card">
<div class="title">Deep Reinforcement Learning for Optimal Portfolio Allocation: A Comparative Study with Mean-Variance Optimization</div>
<div class="meta-line">Authors: Srijan Sood, Kassiani Papasotiriou, Marius Vaiciulis, Tucker Balch</div>
<div class="meta-line">First: 2026-02-19T05:47:23+00:00 · Latest: 2026-02-19T05:47:23+00:00</div>
<div class="meta-line">Comments: 9 pages, 6 figures. Published at the FinPlan&#x27;23 Workshop, the 33rd International Conference on Automated Planning and Scheduling (ICAPS 2023)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17098v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17098v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Portfolio Management is the process of overseeing a group of investments, referred to as a portfolio, with the objective of achieving predetermined investment goals. Portfolio optimization is a key component that involves allocating the portfolio assets so as to maximize returns while minimizing risk taken. It is typically carried out by financial professionals who use a combination of quantitative techniques and investment expertise to make decisions about the portfolio allocation.
  Recent applications of Deep Reinforcement Learning (DRL) have shown promising results when used to optimize portfolio allocation by training model-free agents on historical market data. Many of these methods compare their results against basic benchmarks or other state-of-the-art DRL agents but often fail to compare their performance against traditional methods used by financial professionals in practical settings. One of the most commonly used methods for this task is Mean-Variance Portfolio Optimization (MVO), which uses historical time series information to estimate expected asset returns and covariances, which are then used to optimize for an investment objective.
  Our work is a thorough comparison between model-free DRL and MVO for optimal portfolio allocation. We detail the specifics of how to make DRL for portfolio optimization work in practice, also noting the adjustments needed for MVO. Backtest results demonstrate strong performance of the DRL agent across many metrics, including Sharpe ratio, maximum drawdowns, and absolute returns.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Portfolio Management is the process of overseeing a group of investments, referred to as a portfolio, with the objective of achieving predetermined investment goals.</div>
</details>
</div>
<div class="card">
<div class="title">GraFSTNet: Graph-based Frequency SpatioTemporal Network for Cellular Traffic Prediction</div>
<div class="meta-line">Authors: Ziyi Li, Hui Ma, Fei Xing, Chunjiong Zhang, Ming Yan</div>
<div class="meta-line">First: 2026-02-06T16:21:06+00:00 · Latest: 2026-02-19T05:25:16+00:00</div>
<div class="meta-line">Comments: there exists some small errors in the manuscript, and we would like to check and resubmit later</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13282v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.13282v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With rapid expansion of cellular networks and the proliferation of mobile devices, cellular traffic data exhibits complex temporal dynamics and spatial correlations, posing challenges to accurate traffic prediction. Previous methods often focus predominantly on temporal modeling or depend on predefined spatial topologies, which limits their ability to jointly model spatio-temporal dependencies and effectively capture periodic patterns in cellular traffic. To address these issues, we propose a cellular traffic prediction framework that integrates spatio-temporal modeling with time-frequency analysis. First, we construct a spatial modeling branch to capture inter-cell dependencies through an attention mechanism, minimizing the reliance on predefined topological structures. Second, we build a time-frequency modeling branch to enhance the representation of periodic patterns. Furthermore, we introduce an adaptive-scale LogCosh loss function, which adjusts the error penalty based on traffic magnitude, preventing large errors from dominating the training process and helping the model maintain relatively stable prediction accuracy across different traffic intensities. Experiments on three open-sourced datasets demonstrate that the proposed method achieves prediction performance superior to state-of-the-art approaches.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">With rapid expansion of cellular networks and the proliferation of mobile devices, cellular traffic data exhibits complex temporal dynamics and spatial correlations, posing challenges to accurate traffic prediction.</div>
</details>
</div>
<div class="card">
<div class="title">Spatio-temporal dual-stage hypergraph MARL for human-centric multimodal corridor traffic signal control</div>
<div class="meta-line">Authors: Xiaocai Zhang, Neema Nassir, Milad Haghani</div>
<div class="meta-line">First: 2026-02-19T04:18:50+00:00 · Latest: 2026-02-19T04:18:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17068v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17068v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human-centric traffic signal control in corridor networks must increasingly account for multimodal travelers, particularly high-occupancy public transportation, rather than focusing solely on vehicle-centric performance. This paper proposes STDSH-MARL (Spatio-Temporal Dual-Stage Hypergraph based Multi-Agent Reinforcement Learning), a scalable multi-agent deep reinforcement learning framework that follows a centralized training and decentralized execution paradigm. The proposed method captures spatio-temporal dependencies through a novel dual-stage hypergraph attention mechanism that models interactions across both spatial and temporal hyperedges. In addition, a hybrid discrete action space is introduced to jointly determine the next signal phase configuration and its corresponding green duration, enabling more adaptive signal timing decisions. Experiments conducted on a corridor network under five traffic scenarios demonstrate that STDSH-MARL consistently improves multimodal performance and provides clear benefits for public transportation priority. Compared with state-of-the-art baseline methods, the proposed approach achieves superior overall performance. Further ablation studies confirm the contribution of each component of STDSH-MARL, with temporal hyperedges identified as the most influential factor driving the observed performance gains.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Human-centric traffic signal control in corridor networks must increasingly account for multimodal travelers, particularly high-occupancy public transportation, rather than focusing solely on vehicle-centric performance.</div>
</details>
</div>
<div class="card">
<div class="title">Forecasting Anomaly Precursors via Uncertainty-Aware Time-Series Ensembles</div>
<div class="meta-line">Authors: Hyeongwon Kang, Jinwoo Park, Seunghun Han, Pilsung Kang</div>
<div class="meta-line">First: 2026-02-19T02:48:09+00:00 · Latest: 2026-02-19T02:48:09+00:00</div>
<div class="meta-line">Comments: This manuscript contains 14 pages and 8 figures. It is currently under review at IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17028v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17028v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Detecting anomalies in time-series data is critical in domains such as industrial operations, finance, and cybersecurity, where early identification of abnormal patterns is essential for ensuring system reliability and enabling preventive maintenance. However, most existing methods are reactive: they detect anomalies only after they occur and lack the capability to provide proactive early warning signals. In this paper, we propose FATE (Forecasting Anomalies with Time-series Ensembles), a novel unsupervised framework for detecting Precursors-of-Anomaly (PoA) by quantifying predictive uncertainty from a diverse ensemble of time-series forecasting models. Unlike prior approaches that rely on reconstruction errors or require ground-truth labels, FATE anticipates future values and leverages ensemble disagreement to signal early signs of potential anomalies without access to target values at inference time. To rigorously evaluate PoA detection, we introduce Precursor Time-series Aware Precision and Recall (PTaPR), a new metric that extends the traditional Time-series Aware Precision and Recall (TaPR) by jointly assessing segment-level accuracy, within-segment coverage, and temporal promptness of early predictions. This enables a more holistic assessment of early warning capabilities that existing metrics overlook. Experiments on five real-world benchmark datasets show that FATE achieves an average improvement of 19.9 percentage points in PTaPR AUC and 20.02 percentage points in early detection F1 score, outperforming baselines while requiring no anomaly labels. These results demonstrate the effectiveness and practicality of FATE for real-time unsupervised early warning in complex time-series environments.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Detecting anomalies in time-series data is critical in domains such as industrial operations, finance, and cybersecurity, where early identification of abnormal patterns is essential for ensuring system reliability and enabling preventive maintenance.</div>
</details>
</div>
<div class="card">
<div class="title">Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases</div>
<div class="meta-line">Authors: Zhao Tan, Yiji Zhao, Shiyu Wang, Chang Xu, Yuxuan Liang, Xiping Liu, Shirui Pan, Ming Jin</div>
<div class="meta-line">First: 2026-02-19T01:51:52+00:00 · Latest: 2026-02-19T01:51:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17001v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17001v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Natural Language Querying for Time Series Databases (NLQ4TSDB) aims to assist non-expert users retrieve meaningful events, intervals, and summaries from massive temporal records. However, existing Text-to-SQL methods are not designed for continuous morphological intents such as shapes or anomalies, while time series models struggle to handle ultra-long histories. To address these challenges, we propose Sonar-TS, a neuro-symbolic framework that tackles NLQ4TSDB via a Search-Then-Verify pipeline. Analogous to active sonar, it utilizes a feature index to ping candidate windows via SQL, followed by generated Python programs to lock on and verify candidates against raw signals. To enable effective evaluation, we introduce NLQTSBench, the first large-scale benchmark designed for NLQ over TSDB-scale histories. Our experiments highlight the unique challenges within this domain and demonstrate that Sonar-TS effectively navigates complex temporal queries where traditional methods fail. This work presents the first systematic study of NLQ4TSDB, offering a general framework and evaluation standard to facilitate future research.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Natural Language Querying for Time Series Databases (NLQ4TSDB) aims to assist non-expert users retrieve meaningful events, intervals, and summaries from massive temporal records.</div>
</details>
</div>
<div class="card">
<div class="title">Position: Why a Dynamical Systems Perspective is Needed to Advance Time Series Modeling</div>
<div class="meta-line">Authors: Daniel Durstewitz, Christoph Jürgen Hemmer, Florian Hess, Charlotte Ricarda Doll, Lukas Eisenmann</div>
<div class="meta-line">First: 2026-02-18T20:46:50+00:00 · Latest: 2026-02-18T20:46:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16864v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16864v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Time series (TS) modeling has come a long way from early statistical, mainly linear, approaches to the current trend in TS foundation models. With a lot of hype and industrial demand in this field, it is not always clear how much progress there really is. To advance TS forecasting and analysis to the next level, here we argue that the field needs a dynamical systems (DS) perspective. TS of observations from natural or engineered systems almost always originate from some underlying DS, and arguably access to its governing equations would yield theoretically optimal forecasts. This is the promise of DS reconstruction (DSR), a class of ML/AI approaches that aim to infer surrogate models of the underlying DS from data. But models based on DS principles offer other profound advantages: Beyond short-term forecasts, they enable to predict the long-term statistics of an observed system, which in many practical scenarios may be the more relevant quantities. DS theory furthermore provides domain-independent theoretical insight into mechanisms underlying TS generation, and thereby will inform us, e.g., about upper bounds on performance of any TS model, generalization into unseen regimes as in tipping points, or potential control strategies. After reviewing some of the central concepts, methods, measures, and models in DS theory and DSR, we will discuss how insights from this field can advance TS modeling in crucial ways, enabling better forecasting with much lower computational and memory footprints. We conclude with a number of specific suggestions for translating insights from DSR into TS modeling.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Time series (TS) modeling has come a long way from early statistical, mainly linear, approaches to the current trend in TS foundation models.</div>
</details>
</div>
<div class="card">
<div class="title">A Systematic Evaluation of Sample-Level Tokenization Strategies for MEG Foundation Models</div>
<div class="meta-line">Authors: SungJun Cho, Chetan Gohil, Rukuang Huang, Oiwi Parker Jones, Mark W. Woolrich</div>
<div class="meta-line">First: 2026-02-18T17:21:02+00:00 · Latest: 2026-02-18T17:21:02+00:00</div>
<div class="meta-line">Comments: 15 pages, 10 figures, 1 table</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16626v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16626v1">PDF</a> · <a href="https://github.com/OHBA-analysis/Cho2026_Tokenizer">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent success in natural language processing has motivated growing interest in large-scale foundation models for neuroimaging data. Such models often require discretization of continuous neural time series data, a process referred to as &#x27;tokenization&#x27;. However, the impact of different tokenization strategies for neural data is currently poorly understood. In this work, we present a systematic evaluation of sample-level tokenization strategies for transformer-based large neuroimaging models (LNMs) applied to magnetoencephalography (MEG) data. We compare learnable and non-learnable tokenizers by examining their signal reconstruction fidelity and their impact on subsequent foundation modeling performance (token prediction, biological plausibility of generated data, preservation of subject-specific information, and performance on downstream tasks). For the learnable tokenizer, we introduce a novel approach based on an autoencoder. Experiments were conducted on three publicly available MEG datasets spanning different acquisition sites, scanners, and experimental paradigms. Our results show that both learnable and non-learnable discretization schemes achieve high reconstruction accuracy and broadly comparable performance across most evaluation criteria, suggesting that simple fixed sample-level tokenization strategies can be used in the development of neural foundation models. The code is available at https://github.com/OHBA-analysis/Cho2026_Tokenizer.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent success in natural language processing has motivated growing interest in large-scale foundation models for neuroimaging data.</div>
</details>
</div>
<div class="card">
<div class="title">Still Competitive: Revisiting Recurrent Models for Irregular Time Series Prediction</div>
<div class="meta-line">Authors: Ankitkumar Joshi, Milos Hauskrecht</div>
<div class="meta-line">First: 2025-10-17T19:04:16+00:00 · Latest: 2026-02-18T17:10:14+00:00</div>
<div class="meta-line">Comments: Published in Transactions on Machine Learning Research, 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.16161v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.16161v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modeling irregularly sampled multivariate time series is a persistent challenge in domains like healthcare and sensor networks. While recent works have explored a variety of complex learning architectures to solve the prediction problems for irregularly sampled time series, it remains unclear what the true benefits of some of these architectures are, and whether clever modifications of simpler and more efficient RNN-based algorithms are still competitive, i.e. they are on par with or even superior to these methods. In this work, we propose and study GRUwE: Gated Recurrent Unit with Exponential basis functions, that builds upon RNN-based architectures for observations made at irregular times. GRUwE supports both regression-based and event-based predictions in continuous time. GRUwE works by maintaining a Markov state representation of the time series that updates with the arrival of irregular observations. The Markov state update relies on two reset mechanisms: (i) observation-triggered reset to account for the new observation, and (ii) time-triggered reset that relies on learnable exponential decays, to support the predictions in continuous time. Our empirical evaluations across several real-world benchmarks on next-observation and next-event prediction tasks demonstrate that GRUwE can indeed achieve competitive or superior performance compared to the recent state-of-the-art (SOTA) methods. Thanks to its simplicity, GRUwE offers compelling advantages: it is easy to implement, requires minimal hyper-parameter tuning efforts, and significantly reduces the computational overhead in the online deployment.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Modeling irregularly sampled multivariate time series is a persistent challenge in domains like healthcare and sensor networks.</div>
</details>
</div>
<div class="card">
<div class="title">Channel Dependence, Limited Lookback Windows, and the Simplicity of Datasets: How Biased is Time Series Forecasting?</div>
<div class="meta-line">Authors: Ibram Abdelmalak, Kiran Madhusudhanan, Jungmin Choi, Christian Kloetergens, Vijaya Krishna Yalavarit, Maximilian Stubbemann, Lars Schmidt-Thieme</div>
<div class="meta-line">First: 2025-02-13T13:35:10+00:00 · Latest: 2026-02-18T15:24:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.09683v3">Abs</a> · <a href="https://arxiv.org/pdf/2502.09683v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In Long-term Time Series Forecasting (LTSF), the lookback window is a critical hyperparameter often set arbitrarily, undermining the validity of model evaluations. We argue that the lookback window must be tuned on a per-task basis to ensure fair comparisons. Our empirical results show that failing to do so can invert performance rankings, particularly when comparing univariate and multivariate methods. Experiments on standard benchmarks reposition Channel-Independent (CI) models, such as PatchTST, as state-of-the-art methods. However, we reveal this superior performance is largely an artifact of weak inter-channel correlations and simplicity of patterns within these specific datasets. Using Granger causality analysis and ODE datasets (with implicit channel correlations), we demonstrate that the true strength of multivariate Channel-Dependent (CD) models emerges on datasets with strong, inherent cross-channel dependencies, where they significantly outperform CI models. We conclude with four key recommendations for improving TSF research: (i) consider the lookback window as a key hyperparameter to tune, (ii) for standard datasets, examining CI architectures is advantageous, (iii) leverage statistical analysis of datasets to guide the choice between CI and CD architectures, and (iv) prefer CD models in scenarios with limited data.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In Long-term Time Series Forecasting (LTSF), the lookback window is a critical hyperparameter often set arbitrarily, undermining the validity of model evaluations.</div>
</details>
</div>
<div class="card">
<div class="title">Hardware-accelerated graph neural networks: an alternative approach for neuromorphic event-based audio classification and keyword spotting on SoC FPGA</div>
<div class="meta-line">Authors: Kamil Jeziorek, Piotr Wzorek, Krzysztof Blachut, Hiroshi Nakano, Manon Dampfhoffer, Thomas Mesquida, Hiroaki Nishi, Thomas Dalgaty, Tomasz Kryjak</div>
<div class="meta-line">First: 2026-02-18T13:26:22+00:00 · Latest: 2026-02-18T13:26:22+00:00</div>
<div class="meta-line">Comments: Under revision in TRETS Journal</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16442v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16442v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As the volume of data recorded by embedded edge sensors increases, particularly from neuromorphic devices producing discrete event streams, there is a growing need for hardware-aware neural architectures that enable efficient, low-latency, and energy-conscious local processing. We present an FPGA implementation of event-graph neural networks for audio processing. We utilise an artificial cochlea that converts time-series signals into sparse event data, reducing memory and computation costs. Our architecture was implemented on a SoC FPGA and evaluated on two open-source datasets. For classification task, our baseline floating-point model achieves 92.7% accuracy on SHD dataset - only 2.4% below the state of the art - while requiring over 10x and 67x fewer parameters. On SSC, our models achieve 66.9-71.0% accuracy. Compared to FPGA-based spiking neural networks, our quantised model reaches 92.3% accuracy, outperforming them by up to 19.3% while reducing resource usage and latency. For SSC, we report the first hardware-accelerated evaluation. We further demonstrate the first end-to-end FPGA implementation of event-audio keyword spotting, combining graph convolutional layers with recurrent sequence modelling. The system achieves up to 95% word-end detection accuracy, with only 10.53 microsecond latency and 1.18 W power consumption, establishing a strong benchmark for energy-efficient event-driven KWS.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">As the volume of data recorded by embedded edge sensors increases, particularly from neuromorphic devices producing discrete event streams, there is a growing need for hardware-aware neural architectures that enable efficient, low-latency, and energy-conscious local processing.</div>
</details>
</div>
<div class="card">
<div class="title">A Versatile Variational Quantum Kernel Framework for Non-Trivial Classification</div>
<div class="meta-line">Authors: Jiang Yuhan, Matthew Otten</div>
<div class="meta-line">First: 2025-11-13T22:20:20+00:00 · Latest: 2026-02-18T09:54:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.10831v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.10831v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Quantum kernel methods are a promising branch of quantum machine learning, yet their effectiveness on diverse, high-dimensional, real-world data remains unverified. Current research has largely been limited to low-dimensional or synthetic datasets, preventing a thorough evaluation of their potential. To address this gap, we developed an algorithmic framework for variational quantum kernels utilizing resource-efficient ansätze for complex classification tasks and introduced a parameter scaling technique to accelerate convergence. We conducted a comprehensive benchmark of this framework on eight challenging, real-world and high-dimensional datasets covering tabular, image, time series, and graph data. Our results show that the proposed quantum kernels demonstrate competitive classification accuracy compared to standard classical kernels in classical simulation, such as the radial basis function (RBF) kernel. This work demonstrates that properly designed quantum kernels can function as versatile, high-performance tools, laying a foundation for quantum-enhanced applications in real-world machine learning. Further research is needed to fully assess the practical performance of quantum methods.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Quantum kernel methods are a promising branch of quantum machine learning, yet their effectiveness on diverse, high-dimensional, real-world data remains unverified.</div>
</details>
</div>
<div class="card">
<div class="title">Stochastic Parroting in Temporal Attention -- Regulating the Diagonal Sink</div>
<div class="meta-line">Authors: Victoria Hankemeier, Malte Schilling</div>
<div class="meta-line">First: 2026-02-11T15:45:34+00:00 · Latest: 2026-02-18T08:40:21+00:00</div>
<div class="meta-line">Comments: Accepted at ESANN 2026, Code: https://github.com/vicky-hnk/spatio-temp-parroting</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10956v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.10956v2">PDF</a> · <a href="https://github.com/vicky-hnk/spatio-temp-parroting">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatio-temporal models analyze spatial structures and temporal dynamics, which makes them prone to information degeneration among space and time. Prior literature has demonstrated that over-squashing in causal attention or temporal convolutions creates a bias on the first tokens. To analyze whether such a bias is present in temporal attention mechanisms, we derive sensitivity bounds on the expected value of the Jacobian of a temporal attention layer. We theoretically show how off-diagonal attention scores depend on the sequence length, and that temporal attention matrices suffer a diagonal attention sink. We suggest regularization methods, and experimentally demonstrate their effectiveness.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Spatio-temporal models analyze spatial structures and temporal dynamics, which makes them prone to information degeneration among space and time.</div>
</details>
</div>
<div class="card">
<div class="title">Zero-Shot Temporal Resolution Domain Adaptation for Spiking Neural Networks</div>
<div class="meta-line">Authors: Sanja Karilanova, Maxime Fabre, Emre Neftci, Ayça Özçelikkale</div>
<div class="meta-line">Venue: Neural Networks, 2025, 108483, ISSN 0893-6080</div>
<div class="meta-line">First: 2024-11-07T14:58:51+00:00 · Latest: 2026-02-18T08:38:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.04760v3">Abs</a> · <a href="https://arxiv.org/pdf/2411.04760v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spiking Neural Networks (SNNs) are biologically-inspired deep neural networks that efficiently extract temporal information while offering promising gains in terms of energy efficiency and latency when deployed on neuromorphic devices. SNN parameters are sensitive to temporal resolution, leading to significant performance drops when the temporal resolution of target data during deployment is not the same as that of the source data used for training, especially when fine-tuning with the target data is not possible during deployment. To address this challenge, we propose three novel domain adaptation methods for adapting neuron parameters to account for the change in time resolution without re-training on target time resolution. The proposed methods are based on a mapping between neuron dynamics in SNNs and State Space Models (SSMs) and are applicable to general neuron models. We evaluate the proposed methods under spatio-temporal data tasks, namely the audio keyword spotting datasets SHD and MSWC, and the neuromorphic image NMINST dataset. Our methods provide an alternative to-and in most cases significantly outperform-the existing reference method that consists of scaling only the time constant. Notably, when the temporal resolution of the target data is double that of the source data, applying one of our proposed methods instead of the benchmark achieves classification accuracy of 89.5% instead of 53.0% on SHD, 93.6% instead of 38.8% on MSWC and 98.5% instead of 97.2% aon NMNIST. Moreover, our results show that high accuracy on high temporal resolution data can be obtained by time-efficient training on lower temporal resolution data.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Spiking Neural Networks (SNNs) are biologically-inspired deep neural networks that efficiently extract temporal information while offering promising gains in terms of energy efficiency and latency when deployed on neuromorphic devices.</div>
</details>
</div>
<div class="card">
<div class="title">KnowIt: Deep Time Series Modeling and Interpretation</div>
<div class="meta-line">Authors: M. W. Theunissen, R. Rabe, H. L. Potgieter, M. H. Davel</div>
<div class="meta-line">First: 2025-07-08T14:14:05+00:00 · Latest: 2026-02-18T08:20:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.06009v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.06009v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://must-deep-learning.github.io/KnowIt">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">KnowIt (Knowledge discovery in time series data) is a flexible framework for building deep time series models and interpreting them. It is implemented as a Python toolkit, with source code and documentation available from https://must-deep-learning.github.io/KnowIt. It imposes minimal assumptions about task specifications and decouples the definition of dataset, deep neural network architecture, and interpretability technique through well defined interfaces. This ensures the ease of importing new datasets, custom architectures, and the definition of different interpretability paradigms while maintaining on-the-fly modeling and interpretation of different aspects of a user&#x27;s own time series data. KnowIt aims to provide an environment where users can perform knowledge discovery on their own complex time series data through building powerful deep learning models and explaining their behavior. With ongoing development, collaboration and application our goal is to make this a platform to progress this underexplored field and produce a trusted tool for deep time series modeling.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">KnowIt (Knowledge discovery in time series data) is a flexible framework for building deep time series models and interpreting them.</div>
</details>
</div>
<div class="card">
<div class="title">StarEmbed: Benchmarking Time Series Foundation Models on Astronomical Observations of Variable Stars</div>
<div class="meta-line">Authors: Weijian Li, Hong-Yu Chen, Nabeel Rehemtulla, Ved G. Shah, Dennis Wu, Dongho Kim, Qinjie Lin, Adam A. Miller, Han Liu</div>
<div class="meta-line">First: 2025-10-07T17:53:56+00:00 · Latest: 2026-02-18T07:33:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.06200v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.06200v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Time series foundation models (TSFMs) are increasingly being adopted as highly-capable general-purpose time series representation learners. Although their training corpora are vast, they exclude astronomical time series data. Observations of stars produce peta-scale time series with unique challenges including irregular sampling and heteroskedasticity. We introduce StarEmbed, the first public benchmark for rigorous and standardized evaluation of state-of-the-art TSFMs on stellar time series observations (``light curves&#x27;&#x27;). We benchmark on three scientifically-motivated downstream tasks: unsupervised clustering, supervised classification, and out-of-distribution source detection. StarEmbed integrates a catalog of expert-vetted labels with multi-variate light curves from the Zwicky Transient Facility, yielding ~40k hand-labeled light curves spread across seven astrophysical classes. We evaluate the zero-shot representation capabilities of three TSFMs (MOIRAI, Chronos, Chronos-Bolt) and a domain-specific transformer (Astromer) against handcrafted feature extraction, the long-standing baseline in the astrophysics literature. Our results demonstrate that these TSFMs, especially the Chronos models, which are trained on data completely unlike the astronomical observations, can outperform established astrophysics-specific baselines in some tasks and effectively generalize to entirely new data. In particular, TSFMs deliver state-of-the-art performance on our out-of-distribution source detection benchmark. With the first benchmark of TSFMs on astronomical time series data, we test the limits of their generalization and motivate a paradigm shift in time-domain astronomy from using task-specific, fully supervised pipelines toward adopting generic foundation model representations for the analysis of peta-scale datasets from forthcoming observatories.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Time series foundation models (TSFMs) are increasingly being adopted as highly-capable general-purpose time series representation learners.</div>
</details>
</div>
<div class="card">
<div class="title">Amortized Predictability-aware Training Framework for Time Series Forecasting and Classification</div>
<div class="meta-line">Authors: Xu Zhang, Peng Wang, Yichen Li, Wei Wang</div>
<div class="meta-line">Venue: WWW 2026</div>
<div class="meta-line">First: 2026-02-18T06:59:05+00:00 · Latest: 2026-02-18T06:59:05+00:00</div>
<div class="meta-line">Comments: This work is accepted by the proceedings of the ACM Web Conference 2026 (WWW 2026). The code is available at the link https://github.com/Meteor-Stars/APTF</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16224v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16224v1">PDF</a> · <a href="https://github.com/Meteor-Stars/APTF">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Time series data are prone to noise in various domains, and training samples may contain low-predictability patterns that deviate from the normal data distribution, leading to training instability or convergence to poor local minima. Therefore, mitigating the adverse effects of low-predictability samples is crucial for time series analysis tasks such as time series forecasting (TSF) and time series classification (TSC). While many deep learning models have achieved promising performance, few consider how to identify and penalize low-predictability samples to improve model performance from the training perspective. To fill this gap, we propose a general Amortized Predictability-aware Training Framework (APTF) for both TSF and TSC. APTF introduces two key designs that enable the model to focus on high-predictability samples while still learning appropriately from low-predictability ones: (i) a Hierarchical Predictability-aware Loss (HPL) that dynamically identifies low-predictability samples and progressively expands their loss penalty as training evolves, and (ii) an amortization model that mitigates predictability estimation errors caused by model bias, further enhancing HPL&#x27;s effectiveness. The code is available at https://github.com/Meteor-Stars/APTF.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Time series data are prone to noise in various domains, and training samples may contain low-predictability patterns that deviate from the normal data distribution, leading to training instability or convergence to poor local minima.</div>
</details>
</div>
<div class="card">
<div class="title">SEMixer: Semantics Enhanced MLP-Mixer for Multiscale Mixing and Long-term Time Series Forecasting</div>
<div class="meta-line">Authors: Xu Zhang, Qitong Wang, Peng Wang, Wei Wang</div>
<div class="meta-line">Venue: WWW 2026</div>
<div class="meta-line">First: 2026-02-18T06:53:32+00:00 · Latest: 2026-02-18T06:53:32+00:00</div>
<div class="meta-line">Comments: This work is accepted by the proceedings of the ACM Web Conference 2026 (WWW 2026). The code is available at the link https://github.com/Meteor-Stars/SEMixer</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16220v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16220v1">PDF</a> · <a href="https://github.com/Meteor-Stars/SEMixer">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modeling multiscale patterns is crucial for long-term time series forecasting (TSF). However, redundancy and noise in time series, together with semantic gaps between non-adjacent scales, make the efficient alignment and integration of multi-scale temporal dependencies challenging. To address this, we propose SEMixer, a lightweight multiscale model designed for long-term TSF. SEMixer features two key components: a Random Attention Mechanism (RAM) and a Multiscale Progressive Mixing Chain (MPMC). RAM captures diverse time-patch interactions during training and aggregates them via dropout ensemble at inference, enhancing patch-level semantics and enabling MLP-Mixer to better model multi-scale dependencies. MPMC further stacks RAM and MLP-Mixer in a memory-efficient manner, achieving more effective temporal mixing. It addresses semantic gaps across scales and facilitates better multiscale modeling and forecasting performance. We not only validate the effectiveness of SEMixer on 10 public datasets, but also on the \textit{2025 CCF AlOps Challenge} based on 21GB real wireless network data, where SEMixer achieves third place. The code is available at the link https://github.com/Meteor-Stars/SEMixer.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Modeling multiscale patterns is crucial for long-term time series forecasting (TSF).</div>
</details>
</div>
<div class="card">
<div class="title">Linked Data Classification using Neurochaos Learning</div>
<div class="meta-line">Authors: Pooja Honna, Ayush Patravali, Nithin Nagaraj, Nanjangud C. Narendra</div>
<div class="meta-line">First: 2026-02-18T05:55:59+00:00 · Latest: 2026-02-18T05:55:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16204v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16204v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neurochaos Learning (NL) has shown promise in recent times over traditional deep learning due to its two key features: ability to learn from small sized training samples, and low compute requirements. In prior work, NL has been implemented and extensively tested on separable and time series data, and demonstrated its superior performance on both classification and regression tasks. In this paper, we investigate the next step in NL, viz., applying NL to linked data, in particular, data that is represented in the form of knowledge graphs. We integrate linked data into NL by implementing node aggregation on knowledge graphs, and then feeding the aggregated node features to the simplest NL architecture: ChaosNet. We demonstrate the results of our implementation on homophilic graph datasets as well as heterophilic graph datasets of verying heterophily. We show better efficacy of our approach on homophilic graphs than on heterophilic graphs. While doing so, we also present our analysis of the results, as well as suggestions for future work.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Neurochaos Learning (NL) has shown promise in recent times over traditional deep learning due to its two key features: ability to learn from small sized training samples, and low compute requirements.</div>
</details>
</div>
<div class="card">
<div class="title">Deep TPC: Temporal-Prior Conditioning for Time Series Forecasting</div>
<div class="meta-line">Authors: Filippos Bellos, NaveenJohn Premkumar, Yannis Avrithis, Nam H. Nguyen, Jason J. Corso</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2026-02-18T05:16:29+00:00 · Latest: 2026-02-18T05:16:29+00:00</div>
<div class="meta-line">Comments: Accepted to ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16188v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16188v1">PDF</a> · <a href="https://github.com/fil-mp/Deep_tpc">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM-for-time series (TS) methods typically treat time shallowly, injecting positional or prompt-based cues once at the input of a largely frozen decoder, which limits temporal reasoning as this information degrades through the layers. We introduce Temporal-Prior Conditioning (TPC), which elevates time to a first-class modality that conditions the model at multiple depths. TPC attaches a small set of learnable time series tokens to the patch stream; at selected layers these tokens cross-attend to temporal embeddings derived from compact, human-readable temporal descriptors encoded by the same frozen LLM, then feed temporal context back via self-attention. This disentangles time series signal and temporal information while maintaining a low parameter budget. We show that by training only the cross-attention modules and explicitly disentangling time series signal and temporal information, TPC consistently outperforms both full fine-tuning and shallow conditioning strategies, achieving state-of-the-art performance in long-term forecasting across diverse datasets. Code available at: https://github.com/fil-mp/Deep_tpc</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">LLM-for-time series (TS) methods typically treat time shallowly, injecting positional or prompt-based cues once at the input of a largely frozen decoder, which limits temporal reasoning as this information degrades through the layers.</div>
</details>
</div>
<div class="card">
<div class="title">Uncertainty Matters in Dynamic Gaussian Splatting for Monocular 4D Reconstruction</div>
<div class="meta-line">Authors: Fengzhi Guo, Chih-Chuan Hsu, Sihao Ding, Cheng Zhang</div>
<div class="meta-line">First: 2025-10-14T17:47:11+00:00 · Latest: 2026-02-18T03:18:53+00:00</div>
<div class="meta-line">Comments: Project page: https://tamu-visual-ai.github.io/usplat4d/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.12768v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.12768v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://tamu-visual-ai.github.io/usplat4d/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reconstructing dynamic 3D scenes from monocular input is fundamentally under-constrained, with ambiguities arising from occlusion and extreme novel views. While dynamic Gaussian Splatting offers an efficient representation, vanilla models optimize all Gaussian primitives uniformly, ignoring whether they are well or poorly observed. This limitation leads to motion drifts under occlusion and degraded synthesis when extrapolating to unseen views. We argue that uncertainty matters: Gaussians with recurring observations across views and time act as reliable anchors to guide motion, whereas those with limited visibility are treated as less reliable. To this end, we introduce USplat4D, a novel Uncertainty-aware dynamic Gaussian Splatting framework that propagates reliable motion cues to enhance 4D reconstruction. Our approach estimates time-varying per-Gaussian uncertainty and leverages it to construct a spatio-temporal graph for uncertainty-aware optimization. Experiments on diverse real and synthetic datasets show that explicitly modeling uncertainty consistently improves dynamic Gaussian Splatting models, yielding more stable geometry under occlusion and high-quality synthesis at extreme viewpoints.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Reconstructing dynamic 3D scenes from monocular input is fundamentally under-constrained, with ambiguities arising from occlusion and extreme novel views.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260221_0336.html">20260221_0336</a>
<a href="archive/20260220_0338.html">20260220_0338</a>
<a href="archive/20260219_0348.html">20260219_0348</a>
<a href="archive/20260218_0346.html">20260218_0346</a>
<a href="archive/20260217_0331.html">20260217_0331</a>
<a href="archive/20260216_0324.html">20260216_0324</a>
<a href="archive/20260215_0322.html">20260215_0322</a>
<a href="archive/20260213_0348.html">20260213_0348</a>
<a href="archive/20260212_0355.html">20260212_0355</a>
<a href="archive/20260211_0358.html">20260211_0358</a>
<a href="archive/20260210_0401.html">20260210_0401</a>
<a href="archive/20260208_0323.html">20260208_0323</a>
<a href="archive/20260207_0337.html">20260207_0337</a>
<a href="archive/20260206_0336.html">20260206_0336</a>
<a href="archive/20260205_0338.html">20260205_0338</a>
<a href="archive/20260204_0344.html">20260204_0344</a>
<a href="archive/20260202_0323.html">20260202_0323</a>
<a href="archive/20260201_0320.html">20260201_0320</a>
<a href="archive/20260131_0331.html">20260131_0331</a>
<a href="archive/20260130_0330.html">20260130_0330</a>
<a href="archive/20260129_0327.html">20260129_0327</a>
<a href="archive/20260127_0323.html">20260127_0323</a>
<a href="archive/20260126_0316.html">20260126_0316</a>
<a href="archive/20260125_0316.html">20260125_0316</a>
<a href="archive/20260124_0323.html">20260124_0323</a>
<a href="archive/20260123_0324.html">20260123_0324</a>
<a href="archive/20260122_0326.html">20260122_0326</a>
<a href="archive/20260121_0411.html">20260121_0411</a>
<a href="archive/20260120_0319.html">20260120_0319</a>
<a href="archive/20260119_0315.html">20260119_0315</a>
<a href="archive/20260118_0315.html">20260118_0315</a>
<a href="archive/20260117_0319.html">20260117_0319</a>
<a href="archive/20260116_0323.html">20260116_0323</a>
<a href="archive/20260115_0318.html">20260115_0318</a>
<a href="archive/20260114_0319.html">20260114_0319</a>
<a href="archive/20260113_0320.html">20260113_0320</a>
<a href="archive/20260112_0315.html">20260112_0315</a>
<a href="archive/20260111_0315.html">20260111_0315</a>
<a href="archive/20260110_0319.html">20260110_0319</a>
<a href="archive/20260109_0319.html">20260109_0319</a>
<a href="archive/20260108_0320.html">20260108_0320</a>
<a href="archive/20260107_0316.html">20260107_0316</a>
<a href="archive/20260106_0320.html">20260106_0320</a>
<a href="archive/20260105_0315.html">20260105_0315</a>
<a href="archive/20260104_2357.html">20260104_2357</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
