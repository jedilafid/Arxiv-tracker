<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-06 03:20</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260106_0320</div>
    <div class="row"><div class="card">
<div class="title">BSAT: B-Spline Adaptive Tokenizer for Long-Term Time Series Forecasting</div>
<div class="meta-line">Authors: Maximilian Reinwardt, Michael Eichelbeck, Matthias Althoff</div>
<div class="meta-line">First: 2026-01-02T14:27:54+00:00 · Latest: 2026-01-02T14:27:54+00:00</div>
<div class="meta-line">Comments: 20 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00698v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.00698v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Long-term time series forecasting using transformers is hampered by the quadratic complexity of self-attention and the rigidity of uniform patching, which may be misaligned with the data&#x27;s semantic structure. In this paper, we introduce the \textit{B-Spline Adaptive Tokenizer (BSAT)}, a novel, parameter-free method that adaptively segments a time series by fitting it with B-splines. BSAT algorithmically places tokens in high-curvature regions and represents each variable-length basis function as a fixed-size token, composed of its coefficient and position. Further, we propose a hybrid positional encoding that combines a additive learnable positional encoding with Rotary Positional Embedding featuring a layer-wise learnable base: L-RoPE. This allows each layer to attend to different temporal dependencies. Our experiments on several public benchmarks show that our model is competitive with strong performance at high compression rates. This makes it particularly well-suited for use cases with strong memory constraints.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Long-term time series forecasting using transformers is hampered by the quadratic complexity of self-attention and the rigidity of uniform patching, which may be misaligned with the data&#x27;s semantic structure.</div>
</details>
</div>
<div class="card">
<div class="title">Interpretability-Guided Bi-objective Optimization: Aligning Accuracy and Explainability</div>
<div class="meta-line">Authors: Kasra Fouladi, Hamta Rahmani</div>
<div class="meta-line">First: 2026-01-02T11:32:00+00:00 · Latest: 2026-01-02T11:32:00+00:00</div>
<div class="meta-line">Comments: 10 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00655v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.00655v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper introduces Interpretability-Guided Bi-objective Optimization (IGBO), a framework that trains interpretable models by incorporating structured domain knowledge via a bi-objective formulation. IGBO encodes feature importance hierarchies as a Directed Acyclic Graph (DAG) and uses Temporal Integrated Gradients (TIG) to measure feature importance. To address the Out-of-Distribution (OOD) problem in TIG computation, we propose an Optimal Path Oracle that learns data-manifold-aware integration paths. Theoretical analysis proves convergence properties and robustness to mini-batch noise, while empirical results on time-series data demonstrate IGBO&#x27;s effectiveness in enforcing DAG constraints with minimal accuracy loss, outperforming standard regularization baselines.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces Interpretability-Guided Bi-objective Optimization (IGBO), a framework that trains interpretable models by incorporating structured domain knowledge via a bi-objective formulation.</div>
</details>
</div>
<div class="card">
<div class="title">Causality-Inspired Safe Residual Correction for Multivariate Time Series</div>
<div class="meta-line">Authors: Jianxiang Xie, Yuncheng Hua, Mingyue Cheng, Flora Salim, Hao Xue</div>
<div class="meta-line">First: 2025-12-27T01:34:14+00:00 · Latest: 2026-01-02T05:45:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22428v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.22428v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While modern multivariate forecasters such as Transformers and GNNs achieve strong benchmark performance, they often suffer from systematic errors at specific variables or horizons and, critically, lack guarantees against performance degradation in deployment. Existing post-hoc residual correction methods attempt to fix these errors, but are inherently greedy: although they may improve average accuracy, they can also &quot;help in the wrong way&quot; by overcorrecting reliable predictions and causing local failures in unseen scenarios.
  To address this critical &quot;safety gap,&quot; we propose CRC (Causality-inspired Safe Residual Correction), a plug-and-play framework explicitly designed to ensure non-degradation. CRC follows a divide-and-conquer philosophy: it employs a causality-inspired encoder to expose direction-aware structure by decoupling self- and cross-variable dynamics, and a hybrid corrector to model residual errors. Crucially, the correction process is governed by a strict four-fold safety mechanism that prevents harmful updates.
  Experiments across multiple datasets and forecasting backbones show that CRC consistently improves accuracy, while an in-depth ablation study confirms that its core safety mechanisms ensure exceptionally high non-degradation rates (NDR), making CRC a correction framework suited for safe and reliable deployment.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">While modern multivariate forecasters such as Transformers and GNNs achieve strong benchmark performance, they often suffer from systematic errors at specific variables or horizons and, critically, lack guarantees against performance degradation in deployment.</div>
</details>
</div>
<div class="card">
<div class="title">Learning to be Reproducible: Custom Loss Design for Robust Neural Networks</div>
<div class="meta-line">Authors: Waqas Ahmed, Sheeba Samuel, Kevin Coakley, Birgitta Koenig-Ries, Odd Erik Gundersen</div>
<div class="meta-line">First: 2026-01-02T05:31:08+00:00 · Latest: 2026-01-02T05:31:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00578v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.00578v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To enhance the reproducibility and reliability of deep learning models, we address a critical gap in current training methodologies: the lack of mechanisms that ensure consistent and robust performance across runs. Our empirical analysis reveals that even under controlled initialization and training conditions, the accuracy of the model can exhibit significant variability. To address this issue, we propose a Custom Loss Function (CLF) that reduces the sensitivity of training outcomes to stochastic factors such as weight initialization and data shuffling. By fine-tuning its parameters, CLF explicitly balances predictive accuracy with training stability, leading to more consistent and reliable model performance. Extensive experiments across diverse architectures for both image classification and time series forecasting demonstrate that our approach significantly improves training robustness without sacrificing predictive performance. These results establish CLF as an effective and efficient strategy for developing more stable, reliable and trustworthy neural networks.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enhance the reproducibility and reliability of deep learning models, we address a critical gap in current training methodologies: the lack of mechanisms that ensure consistent and robust performance across runs.</div>
</details>
</div>
<div class="card">
<div class="title">A Comparative Study of Adaptation Strategies for Time Series Foundation Models in Anomaly Detection</div>
<div class="meta-line">Authors: Miseon Park, Kijung Yoon</div>
<div class="meta-line">First: 2026-01-01T19:11:33+00:00 · Latest: 2026-01-01T19:11:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00446v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.00446v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Time series anomaly detection is essential for the reliable operation of complex systems, but most existing methods require extensive task-specific training. We explore whether time series foundation models (TSFMs), pretrained on large heterogeneous data, can serve as universal backbones for anomaly detection. Through systematic experiments across multiple benchmarks, we compare zero-shot inference, full model adaptation, and parameter-efficient fine-tuning (PEFT) strategies. Our results demonstrate that TSFMs outperform task-specific baselines, achieving notable gains in AUC-PR and VUS-PR, particularly under severe class imbalance. Moreover, PEFT methods such as LoRA, OFT, and HRA not only reduce computational cost but also match or surpass full fine-tuning in most cases, indicating that TSFMs can be efficiently adapted for anomaly detection, even when pretrained for forecasting. These findings position TSFMs as promising general-purpose models for scalable and efficient time series anomaly detection.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Time series anomaly detection is essential for the reliable operation of complex systems, but most existing methods require extensive task-specific training.</div>
</details>
</div>
<div class="card">
<div class="title">StockBot 2.0: Vanilla LSTMs Outperform Transformer-based Forecasting for Stock Prices</div>
<div class="meta-line">Authors: Shaswat Mohanty</div>
<div class="meta-line">First: 2026-01-01T04:09:51+00:00 · Latest: 2026-01-01T04:09:51+00:00</div>
<div class="meta-line">Comments: 14 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00197v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.00197v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate forecasting of financial markets remains a long-standing challenge due to complex temporal and often latent dependencies, non-linear dynamics, and high volatility. Building on our earlier recurrent neural network framework, we present an enhanced StockBot architecture that systematically evaluates modern attention-based, convolutional, and recurrent time-series forecasting models within a unified experimental setting. While attention-based and transformer-inspired models offer increased modeling flexibility, extensive empirical evaluation reveals that a carefully constructed vanilla LSTM consistently achieves superior predictive accuracy and more stable buy/sell decision-making when trained under a common set of default hyperparameters. These results highlight the robustness and data efficiency of recurrent sequence models for financial time-series forecasting, particularly in the absence of extensive hyperparameter tuning or the availability of sufficient data when discretized to single-day intervals. Additionally, these results underscore the importance of architectural inductive bias in data-limited market prediction tasks.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Accurate forecasting of financial markets remains a long-standing challenge due to complex temporal and often latent dependencies, non-linear dynamics, and high volatility.</div>
</details>
</div>
<div class="card">
<div class="title">Information-Theoretic Quality Metric of Low-Dimensional Embeddings</div>
<div class="meta-line">Authors: Sebastián Gutiérrez-Bernal, Hector Medel Cobaxin, Abiel Galindo González</div>
<div class="meta-line">First: 2025-12-30T04:34:59+00:00 · Latest: 2026-01-01T02:04:56+00:00</div>
<div class="meta-line">Comments: 18 pages, 6 figures, submitted to Machine Learning (Springer Nature)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23981v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.23981v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this work we study the quality of low-dimensional embeddings from an explicitly information-theoretic perspective. We begin by noting that classical evaluation metrics such as stress, rank-based neighborhood criteria, or Local Procrustes quantify distortions in distances or in local geometries, but do not directly assess how much information is preserved when projecting high-dimensional data onto a lower-dimensional space. To address this limitation, we introduce the Entropy Rank Preservation Measure (ERPM), a local metric based on the Shannon entropy of the singular-value spectrum of neighborhood matrices and on the stable rank, which quantifies changes in uncertainty between the original representation and its reduced projection, providing neighborhood-level indicators and a global summary statistic. To validate the results of the metric, we compare its outcomes with the Mean Relative Rank Error (MRRE), which is distance-based, and with Local Procrustes, which is based on geometric properties, using a financial time series and a manifold commonly studied in the literature. We observe that distance-based criteria exhibit very low correlation with geometric and spectral measures, while ERPM and Local Procrustes show strong average correlation but display significant discrepancies in local regimes, leading to the conclusion that ERPM complements existing metrics by identifying neighborhoods with severe information loss, thereby enabling a more comprehensive assessment of embeddings, particularly in information-sensitive applications such as the construction of early-warning indicators.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In this work we study the quality of low-dimensional embeddings from an explicitly information-theoretic perspective.</div>
</details>
</div>
<div class="card">
<div class="title">Mapping Supraglacial Water as a Window into Surge Hydrology: Linking Surface Water, Drainage Efficiency, and Surge Dynamics on Negribreen, Svalbard</div>
<div class="meta-line">Authors: Rachel Middleton, Ute Herzfeld, Thomas Trantow</div>
<div class="meta-line">First: 2025-12-31T23:14:21+00:00 · Latest: 2025-12-31T23:14:21+00:00</div>
<div class="meta-line">Comments: 53 pages, 16 figures, figures after references</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00137v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.00137v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We analyze the dynamics of Negribreen Glacier System, a polythermal glacier in Svalbard, during its ongoing surge and investigate the role of supraglacial (surface) water as both an indicator of ice-dynamic processes and a driver of surge evolution. We identify three distinct surge phases: the initial acceleration phase, mature phase, and return to quiescence. Comparing the quiescent supraglacial hydrological state to each of the surge phases, we observe a sudden increase in hydrological connectivity between the glacier surface and base during initial acceleration, followed by a gradual return to quiescent water extent. In the mature surge phase, emergent water-filled crevasses coincide with regions of compressive forcing and extensive deformation, follow local accelerations, and preceded smaller, secondary accelerations. Additionaly, rapid drainage of surface ponds is observed in the mature surge. A data-fusion approach, using Maxar WorldView(c) imagery, ICESat-2 altimetry, and Sentinel-1 Synthetic Aperture Radar, is taken to create a time series of supraglacial water maps, water volumes, surface velocity changes, and spatial ice surface roughness. These observations provide a qualitative (process understanding) and quantitative (water time series) basis for supraglacial water sources as a driver and indicator of surge activity for Arctic glaciers.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We analyze the dynamics of Negribreen Glacier System, a polythermal glacier in Svalbard, during its ongoing surge and investigate the role of supraglacial (surface) water as both an indicator of ice-dynamic processes and a driver of surge evolution.</div>
</details>
</div>
<div class="card">
<div class="title">IMBWatch -- a Spatio-Temporal Graph Neural Network approach to detect Illicit Massage Business</div>
<div class="meta-line">Authors: Swetha Varadarajan, Abhishek Ray, Lumina Albert</div>
<div class="meta-line">Venue: AAAI</div>
<div class="meta-line">First: 2025-12-31T19:06:41+00:00 · Latest: 2025-12-31T19:06:41+00:00</div>
<div class="meta-line">Comments: Submitted to AAAI AISI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00075v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.00075v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Illicit Massage Businesses (IMBs) are a covert and persistent form of organized exploitation that operate under the facade of legitimate wellness services while facilitating human trafficking, sexual exploitation, and coerced labor. Detecting IMBs is difficult due to encoded digital advertisements, frequent changes in personnel and locations, and the reuse of shared infrastructure such as phone numbers and addresses. Traditional approaches, including community tips and regulatory inspections, are largely reactive and ineffective at revealing the broader operational networks traffickers rely on.
  To address these challenges, we introduce IMBWatch, a spatio-temporal graph neural network (ST-GNN) framework for large-scale IMB detection. IMBWatch constructs dynamic graphs from open-source intelligence, including scraped online advertisements, business license records, and crowdsourced reviews. Nodes represent heterogeneous entities such as businesses, aliases, phone numbers, and locations, while edges capture spatio-temporal and relational patterns, including co-location, repeated phone usage, and synchronized advertising. The framework combines graph convolutional operations with temporal attention mechanisms to model the evolution of IMB networks over time and space, capturing patterns such as intercity worker movement, burner phone rotation, and coordinated advertising surges.
  Experiments on real-world datasets from multiple U.S. cities show that IMBWatch outperforms baseline models, achieving higher accuracy and F1 scores. Beyond performance gains, IMBWatch offers improved interpretability, providing actionable insights to support proactive and targeted interventions. The framework is scalable, adaptable to other illicit domains, and released with anonymized data and open-source code to support reproducible research.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Illicit Massage Businesses (IMBs) are a covert and persistent form of organized exploitation that operate under the facade of legitimate wellness services while facilitating human trafficking, sexual exploitation, and coerced labor.</div>
</details>
</div>
<div class="card">
<div class="title">AMAP Agentic Planning Technical Report</div>
<div class="meta-line">Authors: Yulan Hu, Xiangwen Zhang, Sheng Ouyang, Hao Yi, Lu Xu, Qinglin Lang, Lide Tan, Xiang Cheng, Tianchen Ye, Zhicong Li, Ge Chen, Wenjin Yang, Zheng Pan, Shaopan Xiong, Siran Yang, Ju Huang, Yan Zhang, Jiamang Wang, Yong Liu, Yinfeng Huang, Tucheng Lin, Xin Li, Ning Guo</div>
<div class="meta-line">First: 2025-12-31T16:39:09+00:00 · Latest: 2025-12-31T16:39:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24957v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24957v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present STAgent, an agentic large language model tailored for spatio-temporal understanding, designed to solve complex tasks such as constrained point-of-interest discovery and itinerary planning. STAgent is a specialized model capable of interacting with ten distinct tools within spatio-temporal scenarios, enabling it to explore, verify, and refine intermediate steps during complex reasoning. Notably, STAgent effectively preserves its general capabilities. We empower STAgent with these capabilities through three key contributions: (1) a stable tool environment that supports over ten domain-specific tools, enabling asynchronous rollout and training; (2) a hierarchical data curation framework that identifies high-quality data like a needle in a haystack, curating high-quality queries with a filter ratio of 1:10,000, emphasizing both diversity and difficulty; and (3) a cascaded training recipe that starts with a seed SFT stage acting as a guardian to measure query difficulty, followed by a second SFT stage fine-tuned on queries with high certainty, and an ultimate RL stage that leverages data of low certainty. Initialized with Qwen3-30B-A3B to establish a strong SFT foundation and leverage insights into sample difficulty, STAgent yields promising performance on TravelBench while maintaining its general capabilities across a wide range of general benchmarks, thereby demonstrating the effectiveness of our proposed agentic model.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present STAgent, an agentic large language model tailored for spatio-temporal understanding, designed to solve complex tasks such as constrained point-of-interest discovery and itinerary planning.</div>
</details>
</div>
<div class="card">
<div class="title">Data-Driven Spectral Analysis Through Pseudo-Resolvent Koopman Operator in Dynamical Systems</div>
<div class="meta-line">Authors: Yuanchao Xu, Itsushi Sakata, Isao Ishikawa</div>
<div class="meta-line">First: 2025-12-31T16:33:01+00:00 · Latest: 2025-12-31T16:33:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24953v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24953v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a data-driven method for spectral analysis of the Koopman operator based on direct construction of the pseudo-resolvent from time-series data. Finite-dimensional approximation of the Koopman operator, such as those obtained from Extended Dynamic Mode Decomposition, are known to suffer from spectral pollution. To address this issue, we construct the pseudo-resolvent operator using the Sherman-Morrison-Woodbury identity whose norm serves as a spectral indicator, and pseudoeigenfunctions are extracted as directions of maximal amplification. We establish convergence of the approximate spectrum to the true spectrum in the Hausdorff metric for isolated eigenvalues, with preservation of algebraic multiplicities, and derive error bounds for eigenvalue approximation. Numerical experiments on pendulum, Lorenz, and coupled oscillator systems demonstrate that the method effectively suppresses spectral pollution and resolves closely spaced spectral components.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present a data-driven method for spectral analysis of the Koopman operator based on direct construction of the pseudo-resolvent from time-series data.</div>
</details>
</div>
<div class="card">
<div class="title">HAROOD: A Benchmark for Out-of-distribution Generalization in Sensor-based Human Activity Recognition</div>
<div class="meta-line">Authors: Wang Lu, Yao Zhu, Jindong Wang</div>
<div class="meta-line">Venue: KDD 2026</div>
<div class="meta-line">First: 2025-12-11T16:52:50+00:00 · Latest: 2025-12-31T15:41:01+00:00</div>
<div class="meta-line">Comments: Accepted by KDD 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10807v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.10807v3">PDF</a> · <a href="https://github.com/AIFrontierLab/HAROOD">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Sensor-based human activity recognition (HAR) mines activity patterns from the time-series sensory data. In realistic scenarios, variations across individuals, devices, environments, and time introduce significant distributional shifts for the same activities. Recent efforts attempt to solve this challenge by applying or adapting existing out-of-distribution (OOD) algorithms, but only in certain distribution shift scenarios (e.g., cross-device or cross-position), lacking comprehensive insights on the effectiveness of these algorithms. For instance, is OOD necessary to HAR? Which OOD algorithm performs the best? In this paper, we fill this gap by proposing HAROOD, a comprehensive benchmark for HAR in OOD settings. We define 4 OOD scenarios: cross-person, cross-position, cross-dataset, and cross-time, and build a testbed covering 6 datasets, 16 comparative methods (implemented with CNN-based and Transformer-based architectures), and two model selection protocols. Then, we conduct extensive experiments and present several findings for future research, e.g., no single method consistently outperforms others, highlighting substantial opportunity for advancement. Our codebase is highly modular and easy to extend for new datasets, algorithms, comparisons, and analysis, with the hope to facilitate the research in OOD-based HAR. Our implementation is released and can be found at https://github.com/AIFrontierLab/HAROOD.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Sensor-based human activity recognition (HAR) mines activity patterns from the time-series sensory data.</div>
</details>
</div>
<div class="card">
<div class="title">PRISM: A hierarchical multiscale approach for time series forecasting</div>
<div class="meta-line">Authors: Zihao Chen, Alexandre Andre, Wenrui Ma, Ian Knight, Sergey Shuvaev, Eva Dyer</div>
<div class="meta-line">First: 2025-12-31T14:51:12+00:00 · Latest: 2025-12-31T14:51:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24898v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24898v1">PDF</a> · <a href="https://github.com/nerdslab/prism">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Forecasting is critical in areas such as finance, biology, and healthcare. Despite the progress in the field, making accurate forecasts remains challenging because real-world time series contain both global trends, local fine-grained structure, and features on multiple scales in between. Here, we present a new forecasting method, PRISM (Partitioned Representation for Iterative Sequence Modeling), that addresses this challenge through a learnable tree-based partitioning of the signal. At the root of the tree, a global representation captures coarse trends in the signal, while recursive splits reveal increasingly localized views of the signal. At each level of the tree, data are projected onto a time-frequency basis (e.g., wavelets or exponential moving averages) to extract scale-specific features, which are then aggregated across the hierarchy. This design allows the model to jointly capture global structure and local dynamics of the signal, enabling accurate forecasting. Experiments across benchmark datasets show that our method outperforms state-of-the-art methods for forecasting. Overall, these results demonstrate that our hierarchical approach provides a lightweight and flexible framework for forecasting multivariate time series. The code is available at https://github.com/nerdslab/prism.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Forecasting is critical in areas such as finance, biology, and healthcare.</div>
</details>
</div>
<div class="card">
<div class="title">Complex variational autoencoders admit Kähler structure</div>
<div class="meta-line">Authors: Andrew Gracyk</div>
<div class="meta-line">First: 2025-11-19T06:51:03+00:00 · Latest: 2025-12-30T23:39:26+00:00</div>
<div class="meta-line">Comments: Fine-tuning; improvements to some technical arguments</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.15172v4">Abs</a> · <a href="https://arxiv.org/pdf/2511.15172v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">It has been discovered that latent-Euclidean variational autoencoders (VAEs) admit, in various capacities, Riemannian structure. We adapt these arguments but for complex VAEs with a complex latent stage. We show that complex VAEs reveal to some level Kähler geometric structure. Our methods will be tailored for decoder geometry. We derive the Fisher information metric in the complex case under a latent complex Gaussian with trivial relation matrix. It is well known from statistical information theory that the Fisher information coincides with the Hessian of the Kullback-Leibler (KL) divergence. Thus, the metric Kähler potential relation is exactly achieved under relative entropy. We propose a Kähler potential derivative of complex Gaussian mixtures that acts as a rough proxy to the Fisher information metric while still being faithful to the underlying Kähler geometry. Computation of the metric via this potential is efficient, and through our potential, valid as a plurisubharmonic (PSH) function, large scale computational burden of automatic differentiation is displaced to small scale. Our methods leverage the law of total covariance to bridge behavior between our potential and the Fisher metric. We show that we can regularize the latent space with decoder geometry, and that we can sample in accordance with a weighted complex volume element. We demonstrate these strategies, at the exchange of sample variation, yield consistently smoother representations and fewer semantic outliers.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">It has been discovered that latent-Euclidean variational autoencoders (VAEs) admit, in various capacities, Riemannian structure.</div>
</details>
</div>
<div class="card">
<div class="title">The Generalization Error of Supervised Machine Learning Algorithms</div>
<div class="meta-line">Authors: Samir M. Perlaza, Xinying Zou</div>
<div class="meta-line">First: 2024-11-18T20:05:51+00:00 · Latest: 2025-12-30T08:11:38+00:00</div>
<div class="meta-line">Comments: Submitted to the IEEE Transaction on Information Theory in November 18, 2024. This version is revision R1 submitted December 30, 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.12030v2">Abs</a> · <a href="https://arxiv.org/pdf/2411.12030v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, the method of gaps, a technique for deriving closed-form expressions in terms of information measures for the generalization error of supervised machine learning algorithms is introduced. The method relies on the notion of \emph{gaps}, which characterize the variation of the expected empirical risk (when either the model or dataset is kept fixed) with respect to changes in the probability measure on the varying parameter (either the dataset or the model, respectively). This distinction results in two classes of gaps: Algorithm-driven gaps (fixed dataset) and data-driven gaps (fixed model). In general, the method relies on two central observations: $(i)$~The generalization error is the expectation of an algorithm-driven gap or a data-driven gap. In the first case, the expectation is with respect to a measure on the datasets; and in the second case, with respect to a measure on the models. $(ii)$~Both, algorithm-driven gaps and data-driven gaps exhibit closed-form expressions in terms of relative entropies. In particular, algorithm-driven gaps involve a Gibbs probability measure on the set of models, which represents a supervised Gibbs algorithm. Alternatively, data-driven gaps involve a worst-case data-generating (WCDG) probability measure on the set of data points, which is also a Gibbs probability measure. Interestingly, such Gibbs measures, which are exogenous to the analysis of generalization, place both the supervised Gibbs algorithm and the WCDG probability measure as natural references for the analysis of supervised learning algorithms. All existing exact expressions for the generalization error of supervised machine learning algorithms can be obtained with the proposed method. Also, this method allows obtaining numerous new exact expressions, which allows establishing connections with other areas in statistics.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In this paper, the method of gaps, a technique for deriving closed-form expressions in terms of information measures for the generalization error of supervised machine learning algorithms is introduced.</div>
</details>
</div>
<div class="card">
<div class="title">RAST: A Retrieval Augmented Spatio-Temporal Framework for Traffic Prediction</div>
<div class="meta-line">Authors: Weilin Ruan, Xilin Dang, Ziyu Zhou, Sisuo Lyu, Yuxuan Liang</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-08-14T10:11:39+00:00 · Latest: 2025-12-30T07:38:32+00:00</div>
<div class="meta-line">Comments: Accepted by AAAI 2026 (AI for Social Impact)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.16623v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.16623v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Traffic prediction is a cornerstone of modern intelligent transportation systems and a critical task in spatio-temporal forecasting. Although advanced Spatio-temporal Graph Neural Networks (STGNNs) and pre-trained models have achieved significant progress in traffic prediction, two key challenges remain: (i) limited contextual capacity when modeling complex spatio-temporal dependencies, and (ii) low predictability at fine-grained spatio-temporal points due to heterogeneous patterns. Inspired by Retrieval-Augmented Generation (RAG), we propose RAST, a universal framework that integrates retrieval-augmented mechanisms with spatio-temporal modeling to address these challenges. Our framework consists of three key designs: 1) Decoupled Encoder and Query Generator to capture decoupled spatial and temporal features and construct a fusion query via residual fusion; 2) Spatio-temporal Retrieval Store and Retrievers to maintain and retrieve vectorized fine-grained patterns; and 3) Universal Backbone Predictor that flexibly accommodates pre-trained STGNNs or simple MLP predictors. Extensive experiments on six real-world traffic networks, including large-scale datasets, demonstrate that RAST achieves superior performance while maintaining computational efficiency.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Traffic prediction is a cornerstone of modern intelligent transportation systems and a critical task in spatio-temporal forecasting.</div>
</details>
</div>
<div class="card">
<div class="title">Anomaly detection in satellite imagery through temporal inpainting</div>
<div class="meta-line">Authors: Bertrand Rouet-Leduc, Claudia Hulbert</div>
<div class="meta-line">First: 2025-12-30T04:58:13+00:00 · Latest: 2025-12-30T04:58:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23986v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23986v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Detecting surface changes from satellite imagery is critical for rapid disaster response and environmental monitoring, yet remains challenging due to the complex interplay between atmospheric noise, seasonal variations, and sensor artifacts. Here we show that deep learning can leverage the temporal redundancy of satellite time series to detect anomalies at unprecedented sensitivity, by learning to predict what the surface should look like in the absence of change. We train an inpainting model built upon the SATLAS foundation model to reconstruct the last frame of a Sentinel-2 time series from preceding acquisitions, using globally distributed training data spanning diverse climate zones and land cover types. When applied to regions affected by sudden surface changes, the discrepancy between prediction and observation reveals anomalies that traditional change detection methods miss. We validate our approach on earthquake-triggered surface ruptures from the 2023 Turkey-Syria earthquake sequence, demonstrating detection of a rift feature in Tepehan with higher sensitivity and specificity than temporal median or Reed-Xiaoli anomaly detectors. Our method reaches detection thresholds approximately three times lower than baseline approaches, providing a path towards automated, global-scale monitoring of surface changes from freely available multi-spectral satellite data.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Detecting surface changes from satellite imagery is critical for rapid disaster response and environmental monitoring, yet remains challenging due to the complex interplay between atmospheric noise, seasonal variations, and sensor artifacts.</div>
</details>
</div>
<div class="card">
<div class="title">Causify DataFlow: A Framework For High-performance Machine Learning Stream Computing</div>
<div class="meta-line">Authors: Giacinto Paolo Saggese, Paul Smith</div>
<div class="meta-line">First: 2025-12-30T04:24:04+00:00 · Latest: 2025-12-30T04:24:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23977v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23977v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present DataFlow, a computational framework for building, testing, and deploying high-performance machine learning systems on unbounded time-series data. Traditional data science workflows assume finite datasets and require substantial reimplementation when moving from batch prototypes to streaming production systems. This gap introduces causality violations, batch boundary artifacts, and poor reproducibility of real-time failures.
  DataFlow resolves these issues through a unified execution model based on directed acyclic graphs (DAGs) with point-in-time idempotency: outputs at any time t depend only on a fixed-length context window preceding t. This guarantee ensures that models developed in batch mode execute identically in streaming production without code changes. The framework enforces strict causality by automatically tracking knowledge time across all transformations, eliminating future-peeking bugs.
  DataFlow supports flexible tiling across temporal and feature dimensions, allowing the same model to operate at different frequencies and memory profiles via configuration alone. It integrates natively with the Python data science stack and provides fit/predict semantics for online learning, caching and incremental computation, and automatic parallelization through DAG-based scheduling. We demonstrate its effectiveness across domains including financial trading, IoT, fraud detection, and real-time analytics.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present DataFlow, a computational framework for building, testing, and deploying high-performance machine learning systems on unbounded time-series data.</div>
</details>
</div>
<div class="card">
<div class="title">STRelay: A Universal Spatio-Temporal Relaying Framework for Location Prediction over Human Trajectory Data</div>
<div class="meta-line">Authors: Bangchao Deng, Lianhua Ji, Chunhua Chen, Xin Jing, Ling Ding, Bingqing QU, Pengyang Wang, Dingqi Yang</div>
<div class="meta-line">First: 2025-08-14T02:44:33+00:00 · Latest: 2025-12-30T03:45:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.16620v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.16620v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Next location prediction is a critical task in human mobility modeling, enabling applications like travel planning and urban mobility management. Existing methods mainly rely on historical spatiotemporal trajectory data to train sequence models that directly forecast future locations. However, they often overlook the importance of the future spatiotemporal contexts, which are highly informative for the future locations. For example, knowing how much time and distance a user will travel could serve as a critical clue for predicting the user&#x27;s next location. Against this background, we propose \textbf{STRelay}, a universal \textbf{\underline{S}}patio\textbf{\underline{T}}emporal \textbf{\underline{Relay}}ing framework explicitly modeling the future spatiotemporal context given a human trajectory, to boost the performance of different location prediction models. Specifically, STRelay models future spatiotemporal contexts in a relaying manner, which is subsequently integrated with the encoded historical representation from a base location prediction model, enabling multi-task learning by simultaneously predicting the next time interval, next moving distance interval, and finally the next location. We evaluate STRelay integrated with five state-of-the-art location prediction base models on four real-world trajectory datasets. Results demonstrate that STRelay consistently improves prediction performance across all cases by 2.49\%-11.30\%. Additionally, we find that the future spatiotemporal contexts are particularly helpful for entertainment-related locations and also for user groups who prefer traveling longer distances. The performance gain on such non-daily-routine activities, which often suffer from higher uncertainty, is indeed complementary to the base location prediction models that often excel at modeling regular daily routine patterns.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Next location prediction is a critical task in human mobility modeling, enabling applications like travel planning and urban mobility management.</div>
</details>
</div>
<div class="card">
<div class="title">Information Physics of Intelligence: Unifying Logical Depth and Entropy under Thermodynamic Constraints</div>
<div class="meta-line">Authors: Jianfeng Xu, Zeyan Li</div>
<div class="meta-line">First: 2025-11-24T14:24:08+00:00 · Latest: 2025-12-30T00:49:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.19156v4">Abs</a> · <a href="https://arxiv.org/pdf/2511.19156v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid scaling of artificial intelligence models has revealed a fundamental tension between model capacity (storage) and inference efficiency (computation). While classical information theory focuses on transmission and storage limits, it lacks a unified physical framework to quantify the thermodynamic costs of generating information from compressed laws versus retrieving it from memory. In this paper, we propose a theoretical framework that treats information processing as an enabling mapping from ontological states to carrier states. We introduce a novel metric, Derivation Entropy, which quantifies the effective work required to compute a target state from a given logical depth. By analyzing the interplay between Shannon entropy (storage) and computational complexity (time/energy), we demonstrate the existence of a critical phase transition point. Below this threshold, memory retrieval is thermodynamically favorable; above it, generative computation becomes the optimal strategy. This &quot;Energy-Time-Space&quot; conservation law provides a physical explanation for the efficiency of generative models and offers a rigorous mathematical bound for designing next-generation, energy-efficient AI architectures. Our findings suggest that the minimization of Derivation Entropy is a governing principle for the evolution of both biological and artificial intelligence.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The rapid scaling of artificial intelligence models has revealed a fundamental tension between model capacity (storage) and inference efficiency (computation).</div>
</details>
</div>
<div class="card">
<div class="title">A multimodal Transformer for InSAR-based ground deformation forecasting with cross-site generalization across Europe</div>
<div class="meta-line">Authors: Wendong Yao, Binhua Huang, Soumyabrata Dev</div>
<div class="meta-line">First: 2025-12-30T00:07:36+00:00 · Latest: 2025-12-30T00:07:36+00:00</div>
<div class="meta-line">Comments: submitted to ISPRS Journal of Photogrammetry and Remote Sensing for review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23906v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23906v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Near-real-time regional-scale monitoring of ground deformation is increasingly required to support urban planning, critical infrastructure management, and natural hazard mitigation. While Interferometric Synthetic Aperture Radar (InSAR) and continental-scale services such as the European Ground Motion Service (EGMS) provide dense observations of past motion, predicting the next observation remains challenging due to the superposition of long-term trends, seasonal cycles, and occasional abrupt discontinuities (e.g., co-seismic steps), together with strong spatial heterogeneity. In this study we propose a multimodal patch-based Transformer for single-step, fixed-interval next-epoch nowcasting of displacement maps from EGMS time series (resampled to a 64x64 grid over 100 km x 100 km tiles). The model ingests recent displacement snapshots together with (i) static kinematic indicators (mean velocity, acceleration, seasonal amplitude) computed in a leakage-safe manner from the training window only, and (ii) harmonic day-of-year encodings. On the eastern Ireland tile (E32N34), the STGCN is strongest in the displacement-only setting, whereas the multimodal Transformer clearly outperforms CNN-LSTM, CNN-LSTM+Attn, and multimodal STGCN when all models receive the same multimodal inputs, achieving RMSE = 0.90 mm and $R^2$ = 0.97 on the test set with the best threshold accuracies.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Near-real-time regional-scale monitoring of ground deformation is increasingly required to support urban planning, critical infrastructure management, and natural hazard mitigation.</div>
</details>
</div>
<div class="card">
<div class="title">Efficient Deep Learning for Short-Term Solar Irradiance Time Series Forecasting: A Benchmark Study in Ho Chi Minh City</div>
<div class="meta-line">Authors: Tin Hoang</div>
<div class="meta-line">First: 2025-12-29T23:22:25+00:00 · Latest: 2025-12-29T23:22:25+00:00</div>
<div class="meta-line">Comments: preprint, 40 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23898v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23898v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reliable forecasting of Global Horizontal Irradiance (GHI) is essential for mitigating the variability of solar energy in power grids. This study presents a comprehensive benchmark of ten deep learning architectures for short-term (1-hour ahead) GHI time series forecasting in Ho Chi Minh City, leveraging high-resolution NSRDB satellite data (2011-2020) to compare established baselines (e.g. LSTM, TCN) against emerging state-of-the-art architectures, including Transformer, Informer, iTransformer, TSMixer, and Mamba. Experimental results identify the Transformer as the superior architecture, achieving the highest predictive accuracy with an R^2 of 0.9696. The study further utilizes SHAP analysis to contrast the temporal reasoning of these architectures, revealing that Transformers exhibit a strong &quot;recency bias&quot; focused on immediate atmospheric conditions, whereas Mamba explicitly leverages 24-hour periodic dependencies to inform predictions. Furthermore, we demonstrate that Knowledge Distillation can compress the high-performance Transformer by 23.5% while surprisingly reducing error (MAE: 23.78 W/m^2), offering a proven pathway for deploying sophisticated, low-latency forecasting on resource-constrained edge devices.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Reliable forecasting of Global Horizontal Irradiance (GHI) is essential for mitigating the variability of solar energy in power grids.</div>
</details>
</div>
<div class="card">
<div class="title">Trellis: Learning to Compress Key-Value Memory in Attention Models</div>
<div class="meta-line">Authors: Mahdi Karami, Ali Behrouz, Praneeth Kacham, Vahab Mirrokni</div>
<div class="meta-line">First: 2025-12-29T20:32:10+00:00 · Latest: 2025-12-29T20:32:10+00:00</div>
<div class="meta-line">Comments: In Second Conference on Language Modeling (COLM) (2025)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23852v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23852v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transformers, while powerful, suffer from quadratic computational complexity and the ever-growing Key-Value (KV) cache of the attention mechanism. This paper introduces Trellis, a novel Transformer architecture with bounded memory that learns how to compress its key-value memory dynamically at test time. Trellis replaces the standard KV cache with a fixed-size memory and train a two-pass recurrent compression mechanism to store new keys and values into memory. To achieve this, it leverages an online gradient descent procedure with a forget gate, enabling the compressed memory to be updated recursively while learning to retain important contextual information from incoming tokens at test time. Extensive experiments on language modeling, common-sense reasoning, recall-intensive tasks, and time series show that the proposed architecture outperforms strong baselines. Notably, its performance gains increase as the sequence length grows, highlighting its potential for long-context applications.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Transformers, while powerful, suffer from quadratic computational complexity and the ever-growing Key-Value (KV) cache of the attention mechanism.</div>
</details>
</div>
<div class="card">
<div class="title">Exploiting the Prior of Generative Time Series Imputation</div>
<div class="meta-line">Authors: YuYang Miao, Chang Li, Zehua Chen</div>
<div class="meta-line">First: 2025-12-29T19:52:15+00:00 · Latest: 2025-12-29T19:52:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23832v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23832v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Time series imputation, i.e., filling the missing values of a time recording, finds various applications in electricity, finance, and weather modelling. Previous methods have introduced generative models such as diffusion probabilistic models and Schrodinger bridge models to conditionally generate the missing values from Gaussian noise or directly from linear interpolation results. However, as their prior is not informative to the ground-truth target, their generation process inevitably suffer increased burden and limited imputation accuracy. In this work, we present Bridge-TS, building a data-to-data generation process for generative time series imputation and exploiting the design of prior with two novel designs. Firstly, we propose expert prior, leveraging a pretrained transformer-based module as an expert to fill the missing values with a deterministic estimation, and then taking the results as the prior of ground truth target. Secondly, we explore compositional priors, utilizing several pretrained models to provide different estimation results, and then combining them in the data-to-data generation process to achieve a compositional priors-to-target imputation process. Experiments conducted on several benchmark datasets such as ETT, Exchange, and Weather show that Bridge-TS reaches a new record of imputation accuracy in terms of mean square error and mean absolute error, demonstrating the superiority of improving prior for generative time series imputation.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Time series imputation, i.e., filling the missing values of a time recording, finds various applications in electricity, finance, and weather modelling.</div>
</details>
</div>
<div class="card">
<div class="title">MS-SSM: A Multi-Scale State Space Model for Efficient Sequence Modeling</div>
<div class="meta-line">Authors: Mahdi Karami, Ali Behrouz, Peilin Zhong, Razvan Pascanu, Vahab Mirrokni</div>
<div class="meta-line">First: 2025-12-29T19:36:28+00:00 · Latest: 2025-12-29T19:36:28+00:00</div>
<div class="meta-line">Comments: In Second Conference on Language Modeling (COLM) (2025)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23824v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23824v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">State-space models (SSMs) have recently attention as an efficient alternative to computationally expensive attention-based models for sequence modeling. They rely on linear recurrences to integrate information over time, enabling fast inference, parallelizable training, and control over recurrence stability. However, traditional SSMs often suffer from limited effective memory, requiring larger state sizes for improved recall. Moreover, existing SSMs struggle to capture multi-scale dependencies, which are essential for modeling complex structures in time series, images, and natural language. This paper introduces a multi-scale SSM framework that addresses these limitations by representing sequence dynamics across multiple resolution and processing each resolution with specialized state-space dynamics. By capturing both fine-grained, high-frequency patterns and coarse, global trends, MS-SSM enhances memory efficiency and long-range modeling. We further introduce an input-dependent scale-mixer, enabling dynamic information fusion across resolutions. The proposed approach significantly improves sequence modeling, particularly in long-range and hierarchical tasks, while maintaining computational efficiency. Extensive experiments on benchmarks, including Long Range Arena, hierarchical reasoning, time series classification, and image recognition, demonstrate that MS-SSM consistently outperforms prior SSM-based models, highlighting the benefits of multi-resolution processing in state-space architectures.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">State-space models (SSMs) have recently attention as an efficient alternative to computationally expensive attention-based models for sequence modeling.</div>
</details>
</div>
<div class="card">
<div class="title">Random Controlled Differential Equations</div>
<div class="meta-line">Authors: Francesco Piatti, Thomas Cass, William F. Turner</div>
<div class="meta-line">First: 2025-12-29T18:25:10+00:00 · Latest: 2025-12-29T18:25:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23670v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23670v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce a training-efficient framework for time-series learning that combines random features with controlled differential equations (CDEs). In this approach, large randomly parameterized CDEs act as continuous-time reservoirs, mapping input paths to rich representations. Only a linear readout layer is trained, resulting in fast, scalable models with strong inductive bias. Building on this foundation, we propose two variants: (i) Random Fourier CDEs (RF-CDEs): these lift the input signal using random Fourier features prior to the dynamics, providing a kernel-free approximation of RBF-enhanced sequence models; (ii) Random Rough DEs (R-RDEs): these operate directly on rough-path inputs via a log-ODE discretization, using log-signatures to capture higher-order temporal interactions while remaining stable and efficient. We prove that in the infinite-width limit, these model induces the RBF-lifted signature kernel and the rough signature kernel, respectively, offering a unified perspective on random-feature reservoirs, continuous-time deep architectures, and path-signature theory.
  We evaluate both models across a range of time-series benchmarks, demonstrating competitive or state-of-the-art performance. These methods provide a practical alternative to explicit signature computations, retaining their inductive bias while benefiting from the efficiency of random features.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We introduce a training-efficient framework for time-series learning that combines random features with controlled differential equations (CDEs).</div>
</details>
</div>
<div class="card">
<div class="title">Forecasting Clinical Risk from Textual Time Series: Structuring Narratives for Temporal AI in Healthcare</div>
<div class="meta-line">Authors: Shahriar Noroozizadeh, Sayantan Kumar, Jeremy C. Weiss</div>
<div class="meta-line">Venue: AAAI</div>
<div class="meta-line">First: 2025-04-14T15:48:56+00:00 · Latest: 2025-12-29T15:21:05+00:00</div>
<div class="meta-line">Comments: AAAI AI for Social Impact 2026. Shahriar Noroozizadeh, Sayantan Kumar (authors contributed equally)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.10340v5">Abs</a> · <a href="https://arxiv.org/pdf/2504.10340v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Clinical case reports encode temporal patient trajectories that are often underexploited by traditional machine learning methods relying on structured data. In this work, we introduce the forecasting problem from textual time series, where timestamped clinical findings -- extracted via an LLM-assisted annotation pipeline -- serve as the primary input for prediction. We systematically evaluate a diverse suite of models, including fine-tuned decoder-based large language models and encoder-based transformers, on tasks of event occurrence prediction, temporal ordering, and survival analysis. Our experiments reveal that encoder-based models consistently achieve higher F1 scores and superior temporal concordance for short- and long-horizon event forecasting, while fine-tuned masking approaches enhance ranking performance. In contrast, instruction-tuned decoder models demonstrate a relative advantage in survival analysis, especially in early prognosis settings. Our sensitivity analyses further demonstrate the importance of time ordering, which requires clinical time series construction, as compared to text ordering, the format of the text inputs that LLMs are classically trained on. This highlights the additional benefit that can be ascertained from time-ordered corpora, with implications for temporal tasks in the era of widespread LLM use.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Clinical case reports encode temporal patient trajectories that are often underexploited by traditional machine learning methods relying on structured data.</div>
</details>
</div>
<div class="card">
<div class="title">Alpha-R1: Alpha Screening with LLM Reasoning via Reinforcement Learning</div>
<div class="meta-line">Authors: Zuoyou Jiang, Li Zhao, Rui Sun, Ruohan Sun, Zhongjian Li, Jing Li, Daxin Jiang, Zuo Bai, Cheng Hua</div>
<div class="meta-line">First: 2025-12-29T14:50:23+00:00 · Latest: 2025-12-29T14:50:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23515v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23515v1">PDF</a> · <a href="https://github.com/FinStep-AI/Alpha-R1">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Signal decay and regime shifts pose recurring challenges for data-driven investment strategies in non-stationary markets. Conventional time-series and machine learning approaches, which rely primarily on historical correlations, often struggle to generalize when the economic environment changes. While large language models (LLMs) offer strong capabilities for processing unstructured information, their potential to support quantitative factor screening through explicit economic reasoning remains underexplored. Existing factor-based methods typically reduce alphas to numerical time series, overlooking the semantic rationale that determines when a factor is economically relevant. We propose Alpha-R1, an 8B-parameter reasoning model trained via reinforcement learning for context-aware alpha screening. Alpha-R1 reasons over factor logic and real-time news to evaluate alpha relevance under changing market conditions, selectively activating or deactivating factors based on contextual consistency. Empirical results across multiple asset pools show that Alpha-R1 consistently outperforms benchmark strategies and exhibits improved robustness to alpha decay. The full implementation and resources are available at https://github.com/FinStep-AI/Alpha-R1.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Signal decay and regime shifts pose recurring challenges for data-driven investment strategies in non-stationary markets.</div>
</details>
</div>
<div class="card">
<div class="title">Eliminating Inductive Bias in Reward Models with Information-Theoretic Guidance</div>
<div class="meta-line">Authors: Zhuo Li, Pengyu Cheng, Zhechao Yu, Feifei Tong, Anningzhe Gao, Tsung-Hui Chang, Xiang Wan, Erchao Zhao, Xiaoxi Jiang, Guanjun Jiang</div>
<div class="meta-line">First: 2025-12-29T13:39:41+00:00 · Latest: 2025-12-29T13:39:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23461v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23461v1">PDF</a> · <a href="https://github.com/Qwen-Applications/DIR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reward models (RMs) are essential in reinforcement learning from human feedback (RLHF) to align large language models (LLMs) with human values. However, RM training data is commonly recognized as low-quality, containing inductive biases that can easily lead to overfitting and reward hacking. For example, more detailed and comprehensive responses are usually human-preferred but with more words, leading response length to become one of the inevitable inductive biases. A limited number of prior RM debiasing approaches either target a single specific type of bias or model the problem with only simple linear correlations, \textit{e.g.}, Pearson coefficients. To mitigate more complex and diverse inductive biases in reward modeling, we introduce a novel information-theoretic debiasing method called \textbf{D}ebiasing via \textbf{I}nformation optimization for \textbf{R}M (DIR). Inspired by the information bottleneck (IB), we maximize the mutual information (MI) between RM scores and human preference pairs, while minimizing the MI between RM outputs and biased attributes of preference inputs. With theoretical justification from information theory, DIR can handle more sophisticated types of biases with non-linear correlations, broadly extending the real-world application scenarios for RM debiasing methods. In experiments, we verify the effectiveness of DIR with three types of inductive biases: \textit{response length}, \textit{sycophancy}, and \textit{format}. We discover that DIR not only effectively mitigates target inductive biases but also enhances RLHF performance across diverse benchmarks, yielding better generalization abilities. The code and training recipes are available at https://github.com/Qwen-Applications/DIR.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Reward models (RMs) are essential in reinforcement learning from human feedback (RLHF) to align large language models (LLMs) with human values.</div>
</details>
</div>
<div class="card">
<div class="title">Deep Generative Models for Synthetic Financial Data: Applications to Portfolio and Risk Modeling</div>
<div class="meta-line">Authors: Christophe D. Hounwanou, Yae Ulrich Gaba</div>
<div class="meta-line">First: 2025-12-25T22:28:32+00:00 · Latest: 2025-12-29T13:33:12+00:00</div>
<div class="meta-line">Comments: 14 pages, submitted as a preprint. This study examines generative models, specifically Time-series Generative Adversarial Networks (TimeGAN) and Variational Autoencoders (VAEs) for creating synthetic financial data to support portfolio construction, trading analysis, and risk modeling</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21798v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.21798v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Synthetic financial data provides a practical solution to the privacy, accessibility, and reproducibility challenges that often constrain empirical research in quantitative finance. This paper investigates the use of deep generative models, specifically Time-series Generative Adversarial Networks (TimeGAN) and Variational Autoencoders (VAEs) to generate realistic synthetic financial return series for portfolio construction and risk modeling applications. Using historical daily returns from the S and P 500 as a benchmark, we generate synthetic datasets under comparable market conditions and evaluate them using statistical similarity metrics, temporal structure tests, and downstream financial tasks. The study shows that TimeGAN produces synthetic data with distributional shapes, volatility patterns, and autocorrelation behaviour that are close to those observed in real returns. When applied to mean--variance portfolio optimization, the resulting synthetic datasets lead to portfolio weights, Sharpe ratios, and risk levels that remain close to those obtained from real data. The VAE provides more stable training but tends to smooth extreme market movements, which affects risk estimation. Finally, the analysis supports the use of synthetic datasets as substitutes for real financial data in portfolio analysis and risk simulation, particularly when models are able to capture temporal dynamics. Synthetic data therefore provides a privacy-preserving, cost-effective, and reproducible tool for financial experimentation and model development.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Synthetic financial data provides a practical solution to the privacy, accessibility, and reproducibility challenges that often constrain empirical research in quantitative finance.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260105_0315.html">20260105_0315</a>
<a href="archive/20260104_2357.html">20260104_2357</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
